{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjwEl5Ars1oFpAZtO/604q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **0.0 Loading Dataset**"
      ],
      "metadata": {
        "id": "qkMtfHS29wQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h4fuIGZJTyAw",
        "outputId": "411fefd1-5585-4b75-df8c-ec9c96e2ba8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 37 (delta 8), reused 7 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (37/37), 321.68 KiB | 6.07 MiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/edwinkmusaasizi/Machine-Learning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Machine-Learning\n",
        "%cd data\n",
        "%cd interim\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOq1g9EKBMoS",
        "outputId": "2517f15a-90b4-4011-85e4-a54700cdb0b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Machine-Learning\n",
            "/content/Machine-Learning/data\n",
            "/content/Machine-Learning/data/interim\n",
            "cleaned_mental_health_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.0 Teacher Model(DNN)**"
      ],
      "metadata": {
        "id": "MbJ-Zgft991a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Data Processing"
      ],
      "metadata": {
        "id": "p1raXwjB_8jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"cleaned_mental_health_data.csv\")\n",
        "\n",
        "# Define adherence labels based on questionnaire responses\n",
        "non_adherence_columns = [\n",
        "    \"Do you ever forget to take your medication?\",\n",
        "    \"Are you careless at times about taking your medication?\",\n",
        "    \"When you feel better, do you sometimes stop taking your medication?\",\n",
        "    \"Sometimes if you feel worse when you take the medication, do you stop taking it?\",\n",
        "    \"I take my medication only when I am sick\"\n",
        "]\n",
        "\n",
        "df[\"adherence\"] = np.where(df[non_adherence_columns].eq(\"Yes\").any(axis=1), 0, 1)\n",
        "\n",
        "# Drop redundant columns\n",
        "df = df.drop(columns=non_adherence_columns + [\"If you have any further comments about medication or this questionnaire, please write them below\"])\n",
        "\n",
        "# Identify all categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "print(\"Categorical columns to encode:\", categorical_cols)\n",
        "\n",
        "# Encode all categorical features\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop(columns=\"adherence\").values\n",
        "y = df[\"adherence\"].values\n",
        "\n",
        "# Split data into train, validation, test (70-15-15)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "azWCwfFIAHap",
        "outputId": "ad8b979d-a49f-4d75-fa21-9478029c5572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns to encode: Index(['sex', 'Religion', 'marital status', 'education status', 'residence',\n",
            "       'substance use', 'comorbidity',\n",
            "       'It is unnatural for my mind and body to be controlled by medication?',\n",
            "       'My thoughts are clearer on medication',\n",
            "       'By staying on medication, I can prevent getting sick',\n",
            "       'I feel weird, like a ‘zombie’ on medication',\n",
            "       'Medication makes me feel tired and sluggish',\n",
            "       'Some of your symptoms are made by your mind.', 'You are mentally well',\n",
            "       'You do not need medication', 'Your stay in the hospital is necessary',\n",
            "       'The doctor is right in prescribing medication for you.',\n",
            "       'You do not need to be seen by a doctor or psychiatrist',\n",
            "       'If someone said you have a nervous or mental illness, they would be right',\n",
            "       'None of the unusual things you are experiencing are due to an illness.',\n",
            "       '. Loss of energy or drive', 'Feeling unmotivated or numb',\n",
            "       'Daytime sedation or drowsiness', 'Sleeping too much',\n",
            "       'Muscles being too tense or stiff', 'Muscles trembling or shaking',\n",
            "       'Feeling restless or jittery',\n",
            "       'Need to move around and pace; inability to sit still',\n",
            "       'Trouble getting to sleep or staying asleep (insomnia)',\n",
            "       'Blurry vision', 'Dry mouth', 'Drooling',\n",
            "       'Memory and concentration problems', 'Constipation', 'Weight changes',\n",
            "       'Changes in sexual functioning', 'Menstrual or breast problem',\n",
            "       'I feel over burdened by the number of pills i swallow per day.',\n",
            "       'How often do yo take your drugs',\n",
            "       'How often do you find no medications in the hospital',\n",
            "       'I am satisfied with doctors explanation about mental illness and the need for treatment',\n",
            "       'Do you sometimes stop your medications because of religious or cultural beliefs'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2  Define the Teacher Model Architecture\n",
        "\n"
      ],
      "metadata": {
        "id": "wsi6nwMMANuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TeacherModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(TeacherModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train.shape[1]\n",
        "teacher = TeacherModel(input_dim)"
      ],
      "metadata": {
        "id": "6TBz1aVfASdg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 Training Loop"
      ],
      "metadata": {
        "id": "kCV4znXeAX2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(teacher.parameters(), lr=0.001)\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "counter = 0\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    teacher.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = teacher(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Validation\n",
        "    teacher.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds, val_true = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = teacher(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_preds.extend(outputs.numpy())\n",
        "            val_true.extend(labels.numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_auc = roc_auc_score(val_true, val_preds)\n",
        "    val_preds_binary = (np.array(val_preds) > 0.5).astype(int)\n",
        "    val_acc = accuracy_score(val_true, val_preds_binary)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(teacher.state_dict(), \"best_teacher.pth\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break"
      ],
      "metadata": {
        "id": "KriFvjVWShcd",
        "outputId": "f79226b7-5513-452a-e2f6-e334dda523db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Train Loss: 0.7060 | Val Loss: 0.6836 | Val AUC: 0.6389 | Val Acc: 0.5000\n",
            "Epoch 2/100\n",
            "Train Loss: 0.6753 | Val Loss: 0.6621 | Val AUC: 0.6806 | Val Acc: 0.6667\n",
            "Epoch 3/100\n",
            "Train Loss: 0.6529 | Val Loss: 0.6412 | Val AUC: 0.7083 | Val Acc: 0.6667\n",
            "Epoch 4/100\n",
            "Train Loss: 0.6379 | Val Loss: 0.6200 | Val AUC: 0.7222 | Val Acc: 0.6667\n",
            "Epoch 5/100\n",
            "Train Loss: 0.6201 | Val Loss: 0.6027 | Val AUC: 0.7222 | Val Acc: 0.6667\n",
            "Epoch 6/100\n",
            "Train Loss: 0.5891 | Val Loss: 0.5912 | Val AUC: 0.7361 | Val Acc: 0.6667\n",
            "Epoch 7/100\n",
            "Train Loss: 0.5650 | Val Loss: 0.5845 | Val AUC: 0.7361 | Val Acc: 0.6667\n",
            "Epoch 8/100\n",
            "Train Loss: 0.5460 | Val Loss: 0.5821 | Val AUC: 0.7222 | Val Acc: 0.6667\n",
            "Epoch 9/100\n",
            "Train Loss: 0.5540 | Val Loss: 0.5811 | Val AUC: 0.7222 | Val Acc: 0.6667\n",
            "Epoch 10/100\n",
            "Train Loss: 0.5286 | Val Loss: 0.5822 | Val AUC: 0.7361 | Val Acc: 0.6667\n",
            "Epoch 11/100\n",
            "Train Loss: 0.4947 | Val Loss: 0.5765 | Val AUC: 0.7361 | Val Acc: 0.6667\n",
            "Epoch 12/100\n",
            "Train Loss: 0.4821 | Val Loss: 0.5740 | Val AUC: 0.7361 | Val Acc: 0.6667\n",
            "Epoch 13/100\n",
            "Train Loss: 0.4620 | Val Loss: 0.5721 | Val AUC: 0.7361 | Val Acc: 0.6667\n",
            "Epoch 14/100\n",
            "Train Loss: 0.4535 | Val Loss: 0.5715 | Val AUC: 0.7361 | Val Acc: 0.6667\n",
            "Epoch 15/100\n",
            "Train Loss: 0.4319 | Val Loss: 0.5778 | Val AUC: 0.7639 | Val Acc: 0.6667\n",
            "Epoch 16/100\n",
            "Train Loss: 0.4121 | Val Loss: 0.5859 | Val AUC: 0.7500 | Val Acc: 0.6667\n",
            "Epoch 17/100\n",
            "Train Loss: 0.3827 | Val Loss: 0.6031 | Val AUC: 0.7500 | Val Acc: 0.6667\n",
            "Epoch 18/100\n",
            "Train Loss: 0.3735 | Val Loss: 0.6253 | Val AUC: 0.7639 | Val Acc: 0.6667\n",
            "Epoch 19/100\n",
            "Train Loss: 0.3456 | Val Loss: 0.6564 | Val AUC: 0.7639 | Val Acc: 0.7222\n",
            "Early stopping!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3.4 Evaluation"
      ],
      "metadata": {
        "id": "lpAgEnrZSmWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "def calculate_specificity(y_true, y_pred):\n",
        "    \"\"\"Calculate specificity (true negative rate).\"\"\"\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    specificity = tn / (tn + fp)\n",
        "    return specificity\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    teacher.train()\n",
        "    train_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = teacher(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Validation\n",
        "    teacher.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds, val_true = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = teacher(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_preds.extend(outputs.numpy())\n",
        "            val_true.extend(labels.numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_loss = train_loss / len(train_loader.dataset)\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "\n",
        "    # Convert predictions to binary (0 or 1)\n",
        "    val_preds_binary = (np.array(val_preds) > 0.5).astype(int)\n",
        "\n",
        "    # Calculate precision, recall, F1-score, specificity, and AUC\n",
        "    val_precision = precision_score(val_true, val_preds_binary)\n",
        "    val_recall = recall_score(val_true, val_preds_binary)\n",
        "    val_f1 = f1_score(val_true, val_preds_binary)\n",
        "    val_specificity = calculate_specificity(val_true, val_preds_binary)\n",
        "    val_auc = roc_auc_score(val_true, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Val Precision: {val_precision:.4f} | Val Recall: {val_recall:.4f}\")\n",
        "    print(f\"Val F1-Score: {val_f1:.4f} | Val Specificity: {val_specificity:.4f}\")\n",
        "    print(f\"Val AUC: {val_auc:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(teacher.state_dict(), \"best_teacher.pth\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "print(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")"
      ],
      "metadata": {
        "id": "LBcGuVmsSs6U",
        "outputId": "73318d03-5283-41d5-ef1d-f87d384c35f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Train Loss: 0.4075 | Val Loss: 0.6106\n",
            "Val Precision: 0.0000 | Val Recall: 0.0000\n",
            "Val F1-Score: 0.0000 | Val Specificity: 1.0000\n",
            "Val AUC: 0.7500\n",
            "Early stopping!\n",
            "Learning Rate: 0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Evaluation"
      ],
      "metadata": {
        "id": "Fon2IdccZaOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best saved model\n",
        "teacher.load_state_dict(torch.load(\"best_teacher.pth\"))\n",
        "\n",
        "# Test evaluation\n",
        "teacher.eval()\n",
        "test_preds, test_true = [], []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = teacher(inputs).squeeze()\n",
        "        test_preds.extend(outputs.numpy())\n",
        "        test_true.extend(labels.numpy())\n",
        "\n",
        "# Convert predictions to binary (0 or 1)\n",
        "test_preds_binary = (np.array(test_preds) > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "test_precision = precision_score(test_true, test_preds_binary)\n",
        "test_recall = recall_score(test_true, test_preds_binary)\n",
        "test_f1 = f1_score(test_true, test_preds_binary)\n",
        "test_specificity = calculate_specificity(test_true, test_preds_binary)\n",
        "test_auc = roc_auc_score(test_true, test_preds)\n",
        "\n",
        "print(\"Test Metrics:\")\n",
        "print(f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f} | Test Specificity: {test_specificity:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")"
      ],
      "metadata": {
        "id": "EA7GuVWmZZ3T",
        "outputId": "f654d1f8-2cf1-464e-a1aa-c970a595bc6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Metrics:\n",
            "Test Precision: 0.0000 | Test Recall: 0.0000\n",
            "Test F1-Score: 0.0000 | Test Specificity: 1.0000\n",
            "Test AUC: 0.4444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-e541cfedddf4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  teacher.load_state_dict(torch.load(\"best_teacher.pth\"))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.0 Contrastive Learning**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-p_QPB9vcUXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Data Preparation for Contrastive Learning"
      ],
      "metadata": {
        "id": "1UxfTJuCdUjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to create pairs of samples for contrastive learning. Each pair consists of:\n",
        "\n",
        "Two samples from the same class (positive pair).\n",
        "\n",
        "Two samples from different classes (negative pair)."
      ],
      "metadata": {
        "id": "pfhpzJTadvdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ContrastiveDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x1 = self.X[idx]\n",
        "        y1 = self.y[idx]\n",
        "\n",
        "        # Randomly select a positive or negative pair\n",
        "        if torch.rand(1) > 0.5:\n",
        "            # Positive pair: same class\n",
        "            idx2 = torch.randint(0, len(self.X), (1,)).item()\n",
        "            while self.y[idx2] != y1:\n",
        "                idx2 = torch.randint(0, len(self.X), (1,)).item()\n",
        "            label = 1  # Positive pair label\n",
        "        else:\n",
        "            # Negative pair: different class\n",
        "            idx2 = torch.randint(0, len(self.X), (1,)).item()\n",
        "            while self.y[idx2] == y1:\n",
        "                idx2 = torch.randint(0, len(self.X), (1,)).item()\n",
        "            label = 0  # Negative pair label\n",
        "\n",
        "        x2 = self.X[idx2]\n",
        "        return x1, x2, label\n",
        "\n",
        "# Create contrastive datasets\n",
        "train_contrastive_dataset = ContrastiveDataset(X_train, y_train)\n",
        "val_contrastive_dataset = ContrastiveDataset(X_val, y_val)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_contrastive_loader = DataLoader(train_contrastive_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_contrastive_loader = DataLoader(val_contrastive_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "X2RJPUf1djeC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Define the Contrastive Loss"
      ],
      "metadata": {
        "id": "zYi7fKaMdyvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use the NT-Xent (Normalized Temperature-Scaled Cross Entropy) loss, which is commonly used in contrastive learning."
      ],
      "metadata": {
        "id": "zvsE2GF5d6na"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z1, z2, labels):\n",
        "        # Normalize the embeddings\n",
        "        z1 = F.normalize(z1, dim=1)\n",
        "        z2 = F.normalize(z2, dim=1)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        sim_matrix = torch.matmul(z1, z2.T) / self.temperature\n",
        "\n",
        "        # Positive pairs are on the diagonal\n",
        "        pos_pairs = torch.diag(sim_matrix)\n",
        "\n",
        "        # Negative pairs are off-diagonal\n",
        "        neg_pairs = sim_matrix[~torch.eye(sim_matrix.size(0), dtype=bool)]\n",
        "\n",
        "        # Compute contrastive loss\n",
        "        pos_loss = -torch.log(torch.exp(pos_pairs) / torch.exp(sim_matrix).sum(dim=1))\n",
        "        neg_loss = -torch.log(1 - torch.exp(neg_pairs) / torch.exp(sim_matrix).sum(dim=1))\n",
        "\n",
        "        # Combine losses\n",
        "        loss = (pos_loss.mean() + neg_loss.mean()) / 2\n",
        "        return loss"
      ],
      "metadata": {
        "id": "8cKKGyJjd-iy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3  Define the Encoder Model"
      ],
      "metadata": {
        "id": "OTo4lU3SeCeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The encoder model will learn meaningful representations of the input data. We’ll use a simple feedforward neural network."
      ],
      "metadata": {
        "id": "PAlob6KRfekd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, output_dim=64):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize encoder\n",
        "input_dim = X_train.shape[1]\n",
        "encoder = Encoder(input_dim)"
      ],
      "metadata": {
        "id": "tr6LLuORffpW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4 Training Loop with Contrastive Learning"
      ],
      "metadata": {
        "id": "c6EoZ0LXfvqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define ContrastiveDataset\n",
        "class ContrastiveDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.FloatTensor(X)  # Convert to FloatTensor\n",
        "        self.y = torch.FloatTensor(y)  # Convert to FloatTensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x1 = self.X[idx]\n",
        "        y1 = self.y[idx]\n",
        "\n",
        "        # Randomly select a positive or negative pair\n",
        "        if torch.rand(1) > 0.5:\n",
        "            # Positive pair: same class\n",
        "            idx2 = torch.randint(0, len(self.X), (1,)).item()\n",
        "            while self.y[idx2] != y1:\n",
        "                idx2 = torch.randint(0, len(self.X), (1,)).item()\n",
        "            label = 1  # Positive pair label\n",
        "        else:\n",
        "            # Negative pair: different class\n",
        "            idx2 = torch.randint(0, len(self.X), (1,)).item()\n",
        "            while self.y[idx2] == y1:\n",
        "                idx2 = torch.randint(0, len(self.X), (1,)).item()\n",
        "            label = 0  # Negative pair label\n",
        "\n",
        "        x2 = self.X[idx2]\n",
        "        return x1, x2, label\n",
        "\n",
        "# Define ContrastiveLoss\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.5):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z1, z2, labels):\n",
        "        # Normalize the embeddings\n",
        "        z1 = F.normalize(z1, dim=1)\n",
        "        z2 = F.normalize(z2, dim=1)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        sim_matrix = torch.matmul(z1, z2.T) / self.temperature\n",
        "\n",
        "        # Positive pairs are on the diagonal\n",
        "        pos_pairs = torch.diag(sim_matrix)\n",
        "\n",
        "        # Negative pairs are off-diagonal\n",
        "        mask = ~torch.eye(sim_matrix.size(0), dtype=bool, device=sim_matrix.device)\n",
        "        neg_pairs = sim_matrix[mask].reshape(sim_matrix.size(0), -1)\n",
        "\n",
        "        # Compute contrastive loss\n",
        "        pos_loss = -torch.log(torch.exp(pos_pairs) / torch.exp(sim_matrix).sum(dim=1))\n",
        "        neg_loss = -torch.log(1 - torch.exp(neg_pairs) / torch.exp(sim_matrix).sum(dim=1).unsqueeze(1))\n",
        "\n",
        "        # Combine losses\n",
        "        loss = (pos_loss.mean() + neg_loss.mean()) / 2\n",
        "        return loss\n",
        "\n",
        "# Define Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, output_dim=64):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize encoder\n",
        "input_dim = X_train.shape[1]\n",
        "encoder = Encoder(input_dim).float()  # Ensure model uses Float\n",
        "\n",
        "# Create contrastive datasets\n",
        "train_contrastive_dataset = ContrastiveDataset(X_train, y_train)\n",
        "val_contrastive_dataset = ContrastiveDataset(X_val, y_val)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_contrastive_loader = DataLoader(train_contrastive_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_contrastive_loader = DataLoader(val_contrastive_dataset, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "# Loss and optimizer\n",
        "contrastive_loss = ContrastiveLoss(temperature=0.5)\n",
        "optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    train_loss = 0.0\n",
        "    for x1, x2, labels in train_contrastive_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        z1 = encoder(x1)\n",
        "        z2 = encoder(x2)\n",
        "\n",
        "        # Compute contrastive loss\n",
        "        loss = contrastive_loss(z1, z2, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * x1.size(0)\n",
        "\n",
        "    # Validation\n",
        "    encoder.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for x1, x2, labels in val_contrastive_loader:\n",
        "            z1 = encoder(x1)\n",
        "            z2 = encoder(x2)\n",
        "            loss = contrastive_loss(z1, z2, labels)\n",
        "            val_loss += loss.item() * x1.size(0)\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = train_loss / len(train_contrastive_loader.dataset)\n",
        "    val_loss = val_loss / len(val_contrastive_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "jDH5Fu5Hf7jN",
        "outputId": "f8e99eb4-9a41-48cb-de7d-3953c04098c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Train Loss: 1.4085 | Val Loss: 0.0000\n",
            "Epoch 2/50\n",
            "Train Loss: 1.3898 | Val Loss: 0.0000\n",
            "Epoch 3/50\n",
            "Train Loss: 1.4124 | Val Loss: 0.0000\n",
            "Epoch 4/50\n",
            "Train Loss: 1.3933 | Val Loss: 0.0000\n",
            "Epoch 5/50\n",
            "Train Loss: 1.3942 | Val Loss: 0.0000\n",
            "Epoch 6/50\n",
            "Train Loss: 1.3854 | Val Loss: 0.0000\n",
            "Epoch 7/50\n",
            "Train Loss: 1.3801 | Val Loss: 0.0000\n",
            "Epoch 8/50\n",
            "Train Loss: 1.3835 | Val Loss: 0.0000\n",
            "Epoch 9/50\n",
            "Train Loss: 1.3995 | Val Loss: 0.0000\n",
            "Epoch 10/50\n",
            "Train Loss: 1.3998 | Val Loss: 0.0000\n",
            "Epoch 11/50\n",
            "Train Loss: 1.3936 | Val Loss: 0.0000\n",
            "Epoch 12/50\n",
            "Train Loss: 1.3963 | Val Loss: 0.0000\n",
            "Epoch 13/50\n",
            "Train Loss: 1.3924 | Val Loss: 0.0000\n",
            "Epoch 14/50\n",
            "Train Loss: 1.3865 | Val Loss: 0.0000\n",
            "Epoch 15/50\n",
            "Train Loss: 1.3857 | Val Loss: 0.0000\n",
            "Epoch 16/50\n",
            "Train Loss: 1.3817 | Val Loss: 0.0000\n",
            "Epoch 17/50\n",
            "Train Loss: 1.3973 | Val Loss: 0.0000\n",
            "Epoch 18/50\n",
            "Train Loss: 1.3879 | Val Loss: 0.0000\n",
            "Epoch 19/50\n",
            "Train Loss: 1.3927 | Val Loss: 0.0000\n",
            "Epoch 20/50\n",
            "Train Loss: 1.3829 | Val Loss: 0.0000\n",
            "Epoch 21/50\n",
            "Train Loss: 1.3830 | Val Loss: 0.0000\n",
            "Epoch 22/50\n",
            "Train Loss: 1.3742 | Val Loss: 0.0000\n",
            "Epoch 23/50\n",
            "Train Loss: 1.3734 | Val Loss: 0.0000\n",
            "Epoch 24/50\n",
            "Train Loss: 1.3824 | Val Loss: 0.0000\n",
            "Epoch 25/50\n",
            "Train Loss: 1.3861 | Val Loss: 0.0000\n",
            "Epoch 26/50\n",
            "Train Loss: 1.3866 | Val Loss: 0.0000\n",
            "Epoch 27/50\n",
            "Train Loss: 1.3884 | Val Loss: 0.0000\n",
            "Epoch 28/50\n",
            "Train Loss: 1.3762 | Val Loss: 0.0000\n",
            "Epoch 29/50\n",
            "Train Loss: 1.3807 | Val Loss: 0.0000\n",
            "Epoch 30/50\n",
            "Train Loss: 1.3796 | Val Loss: 0.0000\n",
            "Epoch 31/50\n",
            "Train Loss: 1.3826 | Val Loss: 0.0000\n",
            "Epoch 32/50\n",
            "Train Loss: 1.3804 | Val Loss: 0.0000\n",
            "Epoch 33/50\n",
            "Train Loss: 1.3906 | Val Loss: 0.0000\n",
            "Epoch 34/50\n",
            "Train Loss: 1.3941 | Val Loss: 0.0000\n",
            "Epoch 35/50\n",
            "Train Loss: 1.3855 | Val Loss: 0.0000\n",
            "Epoch 36/50\n",
            "Train Loss: 1.3853 | Val Loss: 0.0000\n",
            "Epoch 37/50\n",
            "Train Loss: 1.3895 | Val Loss: 0.0000\n",
            "Epoch 38/50\n",
            "Train Loss: 1.3782 | Val Loss: 0.0000\n",
            "Epoch 39/50\n",
            "Train Loss: 1.3858 | Val Loss: 0.0000\n",
            "Epoch 40/50\n",
            "Train Loss: 1.3790 | Val Loss: 0.0000\n",
            "Epoch 41/50\n",
            "Train Loss: 1.3771 | Val Loss: 0.0000\n",
            "Epoch 42/50\n",
            "Train Loss: 1.3836 | Val Loss: 0.0000\n",
            "Epoch 43/50\n",
            "Train Loss: 1.3821 | Val Loss: 0.0000\n",
            "Epoch 44/50\n",
            "Train Loss: 1.3815 | Val Loss: 0.0000\n",
            "Epoch 45/50\n",
            "Train Loss: 1.3834 | Val Loss: 0.0000\n",
            "Epoch 46/50\n",
            "Train Loss: 1.3809 | Val Loss: 0.0000\n",
            "Epoch 47/50\n",
            "Train Loss: 1.3832 | Val Loss: 0.0000\n",
            "Epoch 48/50\n",
            "Train Loss: 1.3780 | Val Loss: 0.0000\n",
            "Epoch 49/50\n",
            "Train Loss: 1.3879 | Val Loss: 0.0000\n",
            "Epoch 50/50\n",
            "Train Loss: 1.3807 | Val Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.5 Use Learned Representations\n"
      ],
      "metadata": {
        "id": "btdTcxyuvgth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract learned representations\n",
        "encoder.eval()\n",
        "with torch.no_grad():\n",
        "    z_train = encoder(torch.FloatTensor(X_train))\n",
        "    z_val = encoder(torch.FloatTensor(X_val))\n",
        "    z_test = encoder(torch.FloatTensor(X_test))"
      ],
      "metadata": {
        "id": "LULIDi4MufqY"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tk34aJCufmL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}