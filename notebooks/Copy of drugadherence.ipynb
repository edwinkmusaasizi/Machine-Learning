{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMGLHbHONe9884fHBvNPZ0A"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **0.0 DATA PROCESSING**"
      ],
      "metadata": {
        "id": "qkMtfHS29wQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h4fuIGZJTyAw",
        "outputId": "60246c36-2868-4bd3-c0c1-419d1777977c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 69 (delta 31), reused 7 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (69/69), 467.68 KiB | 4.37 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/edwinkmusaasizi/Machine-Learning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Machine-Learning\n",
        "%cd data\n",
        "%cd interim\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOq1g9EKBMoS",
        "outputId": "f1322e11-5c9f-452b-9a32-dadeb5dc13f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Machine-Learning/data/interim/Machine-Learning/data/interim/Machine-Learning/data/interim/Machine-Learning\n",
            "/content/Machine-Learning/data/interim/Machine-Learning/data/interim/Machine-Learning/data/interim/Machine-Learning/data\n",
            "/content/Machine-Learning/data/interim/Machine-Learning/data/interim/Machine-Learning/data/interim/Machine-Learning/data/interim\n",
            "cleaned_mental_health_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.1 Data Processing"
      ],
      "metadata": {
        "id": "w3CAYEIG383K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"cleaned_mental_health_data.csv\")\n",
        "\n",
        "# Define adherence labels based on questionnaire responses\n",
        "non_adherence_columns = [\n",
        "    \"Do you ever forget to take your medication?\",\n",
        "    \"Are you careless at times about taking your medication?\",\n",
        "    \"When you feel better, do you sometimes stop taking your medication?\",\n",
        "    \"Sometimes if you feel worse when you take the medication, do you stop taking it?\",\n",
        "    \"I take my medication only when I am sick\"\n",
        "]\n",
        "\n",
        "df[\"adherence\"] = np.where(df[non_adherence_columns].eq(\"Yes\").any(axis=1), 0, 1)\n",
        "\n",
        "# Drop redundant columns\n",
        "df = df.drop(columns=non_adherence_columns + [\"If you have any further comments about medication or this questionnaire, please write them below\"])\n",
        "\n",
        "# Identify all categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "print(\"Categorical columns to encode:\", categorical_cols)\n",
        "\n",
        "# Encode all categorical features\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop(columns=\"adherence\").values\n",
        "y = df[\"adherence\"].values\n",
        "\n",
        "# Split data into train, validation, test (70-15-15)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Apply SMOTE to only the training set\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE only to the training data\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check new class distribution\n",
        "from collections import Counter\n",
        "print(\"New class distribution:\", Counter(y_train_resampled))\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJM_oZTZ5Kyy",
        "outputId": "4add4a18-2ace-49f6-e72c-aef0f70e529d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns to encode: Index(['sex', 'Religion', 'marital status', 'education status', 'residence',\n",
            "       'substance use', 'comorbidity',\n",
            "       'It is unnatural for my mind and body to be controlled by medication?',\n",
            "       'My thoughts are clearer on medication',\n",
            "       'By staying on medication, I can prevent getting sick',\n",
            "       'I feel weird, like a ‘zombie’ on medication',\n",
            "       'Medication makes me feel tired and sluggish',\n",
            "       'Some of your symptoms are made by your mind.', 'You are mentally well',\n",
            "       'You do not need medication', 'Your stay in the hospital is necessary',\n",
            "       'The doctor is right in prescribing medication for you.',\n",
            "       'You do not need to be seen by a doctor or psychiatrist',\n",
            "       'If someone said you have a nervous or mental illness, they would be right',\n",
            "       'None of the unusual things you are experiencing are due to an illness.',\n",
            "       '. Loss of energy or drive', 'Feeling unmotivated or numb',\n",
            "       'Daytime sedation or drowsiness', 'Sleeping too much',\n",
            "       'Muscles being too tense or stiff', 'Muscles trembling or shaking',\n",
            "       'Feeling restless or jittery',\n",
            "       'Need to move around and pace; inability to sit still',\n",
            "       'Trouble getting to sleep or staying asleep (insomnia)',\n",
            "       'Blurry vision', 'Dry mouth', 'Drooling',\n",
            "       'Memory and concentration problems', 'Constipation', 'Weight changes',\n",
            "       'Changes in sexual functioning', 'Menstrual or breast problem',\n",
            "       'I feel over burdened by the number of pills i swallow per day.',\n",
            "       'How often do yo take your drugs',\n",
            "       'How often do you find no medications in the hospital',\n",
            "       'I am satisfied with doctors explanation about mental illness and the need for treatment',\n",
            "       'Do you sometimes stop your medications because of religious or cultural beliefs'],\n",
            "      dtype='object')\n",
            "New class distribution: Counter({1: 55, 0: 55})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution in the training set\n",
        "class_distribution = np.bincount(y_train)\n",
        "print(\"Class Distribution in Training Set:\")\n",
        "print(f\"Class 0 (Non-Adherent): {class_distribution[0]}\")\n",
        "print(f\"Class 1 (Adherent): {class_distribution[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uemTnvGF5Wxd",
        "outputId": "9e4554b3-8778-44f8-fb81-9b92c6747dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution in Training Set:\n",
            "Class 0 (Non-Adherent): 55\n",
            "Class 1 (Adherent): 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement class weight"
      ],
      "metadata": {
        "id": "zSgD2y_45fQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Define loss function with class weights\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Example model (Assuming a simple neural network)\n",
        "class AdherenceModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(AdherenceModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 2)  # Output has 2 classes (0 and 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)  # No softmax needed for CrossEntropyLoss\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[1]  # Number of features\n",
        "model = AdherenceModel(input_size)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (simplified)\n",
        "for epoch in range(10):  # Adjust epochs as needed\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        batch_y = batch_y.long()  # Convert to long for CrossEntropyLoss\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlfiL7ZA5ide",
        "outputId": "7329c1a8-e174-463d-9ab8-fca1cf07bddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6953945159912109\n",
            "Epoch 2, Loss: 0.7195978164672852\n",
            "Epoch 3, Loss: 0.63300621509552\n",
            "Epoch 4, Loss: 0.6330546736717224\n",
            "Epoch 5, Loss: 0.6474331021308899\n",
            "Epoch 6, Loss: 0.5303978323936462\n",
            "Epoch 7, Loss: 0.5786312818527222\n",
            "Epoch 8, Loss: 0.5541043281555176\n",
            "Epoch 9, Loss: 0.5522679090499878\n",
            "Epoch 10, Loss: 0.46876275539398193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M6xRzM2l5T3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL IMPLEMANTION"
      ],
      "metadata": {
        "id": "8jo8AyjIWUam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implement the Feedforward Neural Network"
      ],
      "metadata": {
        "id": "ZBQGUIJkWZSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Implementation"
      ],
      "metadata": {
        "id": "ysSZziXGX0ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the first model architecture (Feedforward Neural Network)\n",
        "class FeedForwardNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(FeedForwardNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)  # First hidden layer\n",
        "        self.fc2 = nn.Linear(64, 32)         # Second hidden layer\n",
        "        self.fc3 = nn.Linear(32, 1)          # Output layer\n",
        "        self.sigmoid = nn.Sigmoid()          # Sigmoid activation for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))           # ReLU activation\n",
        "        x = torch.relu(self.fc2(x))           # ReLU activation\n",
        "        x = self.sigmoid(self.fc3(x))         # Output layer with sigmoid activation\n",
        "        return x\n",
        "\n",
        "# Example input dimension (you should use the actual number of features)\n",
        "input_dim = X_train.shape[1]  # assuming X_train is already defined\n",
        "model = FeedForwardNN(input_dim)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=100):\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs.squeeze(), labels)  # Squeeze to make outputs the same shape as labels\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            correct += (predicted.squeeze() == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        # Validation step\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                val_loss += loss_fn(outputs.squeeze(), labels).item()\n",
        "\n",
        "                predicted = (outputs > 0.5).float()\n",
        "                val_correct += (predicted.squeeze() == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_accuracy = correct / total\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = val_correct / val_total\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "        # Save model if it's the best validation loss\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), \"best_feedforward_model.pth\")\n",
        "\n",
        "# Move the model to the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, optimizer, loss_fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5w4eM-WXXR5",
        "outputId": "b5eba92a-beb0-4b9e-e477-5a5428b83547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6768, Train Accuracy: 0.6667, Val Loss: 0.6705, Val Accuracy: 0.7222\n",
            "Epoch 2/100, Train Loss: 0.6617, Train Accuracy: 0.6790, Val Loss: 0.6587, Val Accuracy: 0.6667\n",
            "Epoch 3/100, Train Loss: 0.6498, Train Accuracy: 0.6790, Val Loss: 0.6482, Val Accuracy: 0.6667\n",
            "Epoch 4/100, Train Loss: 0.6388, Train Accuracy: 0.6790, Val Loss: 0.6389, Val Accuracy: 0.6667\n",
            "Epoch 5/100, Train Loss: 0.6285, Train Accuracy: 0.6790, Val Loss: 0.6297, Val Accuracy: 0.6667\n",
            "Epoch 6/100, Train Loss: 0.6125, Train Accuracy: 0.6790, Val Loss: 0.6206, Val Accuracy: 0.6667\n",
            "Epoch 7/100, Train Loss: 0.5985, Train Accuracy: 0.6790, Val Loss: 0.6110, Val Accuracy: 0.6667\n",
            "Epoch 8/100, Train Loss: 0.5843, Train Accuracy: 0.6790, Val Loss: 0.6006, Val Accuracy: 0.6667\n",
            "Epoch 9/100, Train Loss: 0.5823, Train Accuracy: 0.6914, Val Loss: 0.5890, Val Accuracy: 0.6667\n",
            "Epoch 10/100, Train Loss: 0.5599, Train Accuracy: 0.7037, Val Loss: 0.5770, Val Accuracy: 0.7222\n",
            "Epoch 11/100, Train Loss: 0.5244, Train Accuracy: 0.7407, Val Loss: 0.5651, Val Accuracy: 0.7222\n",
            "Epoch 12/100, Train Loss: 0.5344, Train Accuracy: 0.7531, Val Loss: 0.5520, Val Accuracy: 0.7222\n",
            "Epoch 13/100, Train Loss: 0.5100, Train Accuracy: 0.7901, Val Loss: 0.5398, Val Accuracy: 0.7222\n",
            "Epoch 14/100, Train Loss: 0.4904, Train Accuracy: 0.8395, Val Loss: 0.5281, Val Accuracy: 0.7222\n",
            "Epoch 15/100, Train Loss: 0.4704, Train Accuracy: 0.8642, Val Loss: 0.5172, Val Accuracy: 0.7222\n",
            "Epoch 16/100, Train Loss: 0.4463, Train Accuracy: 0.8889, Val Loss: 0.5069, Val Accuracy: 0.7222\n",
            "Epoch 17/100, Train Loss: 0.4198, Train Accuracy: 0.9012, Val Loss: 0.4970, Val Accuracy: 0.7222\n",
            "Epoch 18/100, Train Loss: 0.4131, Train Accuracy: 0.9136, Val Loss: 0.4888, Val Accuracy: 0.7222\n",
            "Epoch 19/100, Train Loss: 0.3653, Train Accuracy: 0.9136, Val Loss: 0.4825, Val Accuracy: 0.8333\n",
            "Epoch 20/100, Train Loss: 0.3530, Train Accuracy: 0.9259, Val Loss: 0.4772, Val Accuracy: 0.8333\n",
            "Epoch 21/100, Train Loss: 0.3193, Train Accuracy: 0.9506, Val Loss: 0.4727, Val Accuracy: 0.8333\n",
            "Epoch 22/100, Train Loss: 0.3049, Train Accuracy: 0.9630, Val Loss: 0.4697, Val Accuracy: 0.8333\n",
            "Epoch 23/100, Train Loss: 0.2842, Train Accuracy: 0.9630, Val Loss: 0.4674, Val Accuracy: 0.8333\n",
            "Epoch 24/100, Train Loss: 0.2547, Train Accuracy: 0.9630, Val Loss: 0.4672, Val Accuracy: 0.8333\n",
            "Epoch 25/100, Train Loss: 0.2368, Train Accuracy: 0.9630, Val Loss: 0.4685, Val Accuracy: 0.8333\n",
            "Epoch 26/100, Train Loss: 0.2275, Train Accuracy: 0.9630, Val Loss: 0.4711, Val Accuracy: 0.8333\n",
            "Epoch 27/100, Train Loss: 0.1874, Train Accuracy: 0.9630, Val Loss: 0.4763, Val Accuracy: 0.8333\n",
            "Epoch 28/100, Train Loss: 0.1777, Train Accuracy: 0.9630, Val Loss: 0.4817, Val Accuracy: 0.8333\n",
            "Epoch 29/100, Train Loss: 0.1645, Train Accuracy: 0.9877, Val Loss: 0.4877, Val Accuracy: 0.8333\n",
            "Epoch 30/100, Train Loss: 0.1393, Train Accuracy: 0.9877, Val Loss: 0.4947, Val Accuracy: 0.8333\n",
            "Epoch 31/100, Train Loss: 0.1222, Train Accuracy: 0.9877, Val Loss: 0.5009, Val Accuracy: 0.8333\n",
            "Epoch 32/100, Train Loss: 0.1148, Train Accuracy: 0.9877, Val Loss: 0.5088, Val Accuracy: 0.8333\n",
            "Epoch 33/100, Train Loss: 0.0983, Train Accuracy: 0.9877, Val Loss: 0.5167, Val Accuracy: 0.8333\n",
            "Epoch 34/100, Train Loss: 0.0905, Train Accuracy: 0.9877, Val Loss: 0.5262, Val Accuracy: 0.8333\n",
            "Epoch 35/100, Train Loss: 0.0820, Train Accuracy: 1.0000, Val Loss: 0.5362, Val Accuracy: 0.8333\n",
            "Epoch 36/100, Train Loss: 0.0707, Train Accuracy: 1.0000, Val Loss: 0.5457, Val Accuracy: 0.8333\n",
            "Epoch 37/100, Train Loss: 0.0645, Train Accuracy: 1.0000, Val Loss: 0.5550, Val Accuracy: 0.8333\n",
            "Epoch 38/100, Train Loss: 0.0505, Train Accuracy: 1.0000, Val Loss: 0.5637, Val Accuracy: 0.8333\n",
            "Epoch 39/100, Train Loss: 0.0467, Train Accuracy: 1.0000, Val Loss: 0.5738, Val Accuracy: 0.8333\n",
            "Epoch 40/100, Train Loss: 0.0403, Train Accuracy: 1.0000, Val Loss: 0.5823, Val Accuracy: 0.8333\n",
            "Epoch 41/100, Train Loss: 0.0373, Train Accuracy: 1.0000, Val Loss: 0.5911, Val Accuracy: 0.8333\n",
            "Epoch 42/100, Train Loss: 0.0339, Train Accuracy: 1.0000, Val Loss: 0.5980, Val Accuracy: 0.8333\n",
            "Epoch 43/100, Train Loss: 0.0293, Train Accuracy: 1.0000, Val Loss: 0.6052, Val Accuracy: 0.8333\n",
            "Epoch 44/100, Train Loss: 0.0270, Train Accuracy: 1.0000, Val Loss: 0.6133, Val Accuracy: 0.8333\n",
            "Epoch 45/100, Train Loss: 0.0241, Train Accuracy: 1.0000, Val Loss: 0.6205, Val Accuracy: 0.8333\n",
            "Epoch 46/100, Train Loss: 0.0221, Train Accuracy: 1.0000, Val Loss: 0.6279, Val Accuracy: 0.8333\n",
            "Epoch 47/100, Train Loss: 0.0203, Train Accuracy: 1.0000, Val Loss: 0.6356, Val Accuracy: 0.8333\n",
            "Epoch 48/100, Train Loss: 0.0179, Train Accuracy: 1.0000, Val Loss: 0.6427, Val Accuracy: 0.8333\n",
            "Epoch 49/100, Train Loss: 0.0161, Train Accuracy: 1.0000, Val Loss: 0.6491, Val Accuracy: 0.8333\n",
            "Epoch 50/100, Train Loss: 0.0144, Train Accuracy: 1.0000, Val Loss: 0.6554, Val Accuracy: 0.8333\n",
            "Epoch 51/100, Train Loss: 0.0152, Train Accuracy: 1.0000, Val Loss: 0.6618, Val Accuracy: 0.8333\n",
            "Epoch 52/100, Train Loss: 0.0126, Train Accuracy: 1.0000, Val Loss: 0.6681, Val Accuracy: 0.8333\n",
            "Epoch 53/100, Train Loss: 0.0116, Train Accuracy: 1.0000, Val Loss: 0.6747, Val Accuracy: 0.8333\n",
            "Epoch 54/100, Train Loss: 0.0114, Train Accuracy: 1.0000, Val Loss: 0.6814, Val Accuracy: 0.8333\n",
            "Epoch 55/100, Train Loss: 0.0104, Train Accuracy: 1.0000, Val Loss: 0.6882, Val Accuracy: 0.8333\n",
            "Epoch 56/100, Train Loss: 0.0094, Train Accuracy: 1.0000, Val Loss: 0.6946, Val Accuracy: 0.8333\n",
            "Epoch 57/100, Train Loss: 0.0085, Train Accuracy: 1.0000, Val Loss: 0.7007, Val Accuracy: 0.8333\n",
            "Epoch 58/100, Train Loss: 0.0080, Train Accuracy: 1.0000, Val Loss: 0.7066, Val Accuracy: 0.8333\n",
            "Epoch 59/100, Train Loss: 0.0078, Train Accuracy: 1.0000, Val Loss: 0.7116, Val Accuracy: 0.8333\n",
            "Epoch 60/100, Train Loss: 0.0069, Train Accuracy: 1.0000, Val Loss: 0.7164, Val Accuracy: 0.8333\n",
            "Epoch 61/100, Train Loss: 0.0065, Train Accuracy: 1.0000, Val Loss: 0.7210, Val Accuracy: 0.8333\n",
            "Epoch 62/100, Train Loss: 0.0064, Train Accuracy: 1.0000, Val Loss: 0.7252, Val Accuracy: 0.8333\n",
            "Epoch 63/100, Train Loss: 0.0064, Train Accuracy: 1.0000, Val Loss: 0.7298, Val Accuracy: 0.8333\n",
            "Epoch 64/100, Train Loss: 0.0059, Train Accuracy: 1.0000, Val Loss: 0.7335, Val Accuracy: 0.8333\n",
            "Epoch 65/100, Train Loss: 0.0058, Train Accuracy: 1.0000, Val Loss: 0.7378, Val Accuracy: 0.8333\n",
            "Epoch 66/100, Train Loss: 0.0055, Train Accuracy: 1.0000, Val Loss: 0.7418, Val Accuracy: 0.8333\n",
            "Epoch 67/100, Train Loss: 0.0050, Train Accuracy: 1.0000, Val Loss: 0.7462, Val Accuracy: 0.8333\n",
            "Epoch 68/100, Train Loss: 0.0051, Train Accuracy: 1.0000, Val Loss: 0.7508, Val Accuracy: 0.8333\n",
            "Epoch 69/100, Train Loss: 0.0046, Train Accuracy: 1.0000, Val Loss: 0.7557, Val Accuracy: 0.8333\n",
            "Epoch 70/100, Train Loss: 0.0044, Train Accuracy: 1.0000, Val Loss: 0.7605, Val Accuracy: 0.8333\n",
            "Epoch 71/100, Train Loss: 0.0043, Train Accuracy: 1.0000, Val Loss: 0.7648, Val Accuracy: 0.8333\n",
            "Epoch 72/100, Train Loss: 0.0041, Train Accuracy: 1.0000, Val Loss: 0.7684, Val Accuracy: 0.8333\n",
            "Epoch 73/100, Train Loss: 0.0041, Train Accuracy: 1.0000, Val Loss: 0.7720, Val Accuracy: 0.8333\n",
            "Epoch 74/100, Train Loss: 0.0035, Train Accuracy: 1.0000, Val Loss: 0.7762, Val Accuracy: 0.8333\n",
            "Epoch 75/100, Train Loss: 0.0037, Train Accuracy: 1.0000, Val Loss: 0.7802, Val Accuracy: 0.8333\n",
            "Epoch 76/100, Train Loss: 0.0035, Train Accuracy: 1.0000, Val Loss: 0.7839, Val Accuracy: 0.8333\n",
            "Epoch 77/100, Train Loss: 0.0034, Train Accuracy: 1.0000, Val Loss: 0.7872, Val Accuracy: 0.8333\n",
            "Epoch 78/100, Train Loss: 0.0034, Train Accuracy: 1.0000, Val Loss: 0.7901, Val Accuracy: 0.8333\n",
            "Epoch 79/100, Train Loss: 0.0031, Train Accuracy: 1.0000, Val Loss: 0.7938, Val Accuracy: 0.8333\n",
            "Epoch 80/100, Train Loss: 0.0030, Train Accuracy: 1.0000, Val Loss: 0.7971, Val Accuracy: 0.8333\n",
            "Epoch 81/100, Train Loss: 0.0029, Train Accuracy: 1.0000, Val Loss: 0.8004, Val Accuracy: 0.8333\n",
            "Epoch 82/100, Train Loss: 0.0028, Train Accuracy: 1.0000, Val Loss: 0.8037, Val Accuracy: 0.8333\n",
            "Epoch 83/100, Train Loss: 0.0029, Train Accuracy: 1.0000, Val Loss: 0.8070, Val Accuracy: 0.8333\n",
            "Epoch 84/100, Train Loss: 0.0027, Train Accuracy: 1.0000, Val Loss: 0.8104, Val Accuracy: 0.8333\n",
            "Epoch 85/100, Train Loss: 0.0027, Train Accuracy: 1.0000, Val Loss: 0.8135, Val Accuracy: 0.8333\n",
            "Epoch 86/100, Train Loss: 0.0026, Train Accuracy: 1.0000, Val Loss: 0.8165, Val Accuracy: 0.8333\n",
            "Epoch 87/100, Train Loss: 0.0025, Train Accuracy: 1.0000, Val Loss: 0.8192, Val Accuracy: 0.8333\n",
            "Epoch 88/100, Train Loss: 0.0025, Train Accuracy: 1.0000, Val Loss: 0.8218, Val Accuracy: 0.8333\n",
            "Epoch 89/100, Train Loss: 0.0021, Train Accuracy: 1.0000, Val Loss: 0.8248, Val Accuracy: 0.8333\n",
            "Epoch 90/100, Train Loss: 0.0022, Train Accuracy: 1.0000, Val Loss: 0.8274, Val Accuracy: 0.8333\n",
            "Epoch 91/100, Train Loss: 0.0022, Train Accuracy: 1.0000, Val Loss: 0.8300, Val Accuracy: 0.8333\n",
            "Epoch 92/100, Train Loss: 0.0021, Train Accuracy: 1.0000, Val Loss: 0.8328, Val Accuracy: 0.8333\n",
            "Epoch 93/100, Train Loss: 0.0021, Train Accuracy: 1.0000, Val Loss: 0.8351, Val Accuracy: 0.8333\n",
            "Epoch 94/100, Train Loss: 0.0020, Train Accuracy: 1.0000, Val Loss: 0.8375, Val Accuracy: 0.8333\n",
            "Epoch 95/100, Train Loss: 0.0020, Train Accuracy: 1.0000, Val Loss: 0.8397, Val Accuracy: 0.8333\n",
            "Epoch 96/100, Train Loss: 0.0020, Train Accuracy: 1.0000, Val Loss: 0.8419, Val Accuracy: 0.8333\n",
            "Epoch 97/100, Train Loss: 0.0019, Train Accuracy: 1.0000, Val Loss: 0.8442, Val Accuracy: 0.8333\n",
            "Epoch 98/100, Train Loss: 0.0018, Train Accuracy: 1.0000, Val Loss: 0.8467, Val Accuracy: 0.8333\n",
            "Epoch 99/100, Train Loss: 0.0018, Train Accuracy: 1.0000, Val Loss: 0.8493, Val Accuracy: 0.8333\n",
            "Epoch 100/100, Train Loss: 0.0017, Train Accuracy: 1.0000, Val Loss: 0.8516, Val Accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Prepare Data"
      ],
      "metadata": {
        "id": "x2CnxVsQYENi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"cleaned_mental_health_data.csv\")\n",
        "\n",
        "# Define adherence labels based on questionnaire responses\n",
        "non_adherence_columns = [\n",
        "    \"Do you ever forget to take your medication?\",\n",
        "    \"Are you careless at times about taking your medication?\",\n",
        "    \"When you feel better, do you sometimes stop taking your medication?\",\n",
        "    \"Sometimes if you feel worse when you take the medication, do you stop taking it?\",\n",
        "    \"I take my medication only when I am sick\"\n",
        "]\n",
        "\n",
        "df[\"adherence\"] = np.where(df[non_adherence_columns].eq(\"Yes\").any(axis=1), 0, 1)\n",
        "\n",
        "# Drop redundant columns\n",
        "df = df.drop(columns=non_adherence_columns + [\"If you have any further comments about medication or this questionnaire, please write them below\"])\n",
        "\n",
        "# Identify all categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "print(\"Categorical columns to encode:\", categorical_cols)\n",
        "\n",
        "# Encode all categorical features\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop(columns=\"adherence\").values\n",
        "y = df[\"adherence\"].values\n",
        "\n",
        "# Split data into train, validation, test (70-15-15)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Apply SMOTE to only the training set\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE only to the training data\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check new class distribution\n",
        "from collections import Counter\n",
        "print(\"New class distribution:\", Counter(y_train_resampled))\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Check the shape of the data to ensure it matches\n",
        "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
        "print(\"Shape of y_train_resampled:\", y_train_resampled.shape)\n",
        "\n",
        "# Ensure that X_train_scaled and y_train_resampled have the same number of samples\n",
        "assert X_train_scaled.shape[0] == y_train_resampled.shape[0], \"Mismatch in number of samples between X_train_scaled and y_train_resampled\"\n",
        "\n",
        "# Convert to torch tensors\n",
        "train_tensor = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32),\n",
        "                              torch.tensor(y_train_resampled, dtype=torch.float32))\n",
        "val_tensor = TensorDataset(torch.tensor(X_val_scaled, dtype=torch.float32),\n",
        "                            torch.tensor(y_val, dtype=torch.float32))\n",
        "test_tensor = TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32),\n",
        "                             torch.tensor(y_test, dtype=torch.float32))\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_tensor, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_tensor, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4mfmtVAYKJC",
        "outputId": "b420c173-10a0-4e87-8e85-d11a85543c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns to encode: Index(['sex', 'Religion', 'marital status', 'education status', 'residence',\n",
            "       'substance use', 'comorbidity',\n",
            "       'It is unnatural for my mind and body to be controlled by medication?',\n",
            "       'My thoughts are clearer on medication',\n",
            "       'By staying on medication, I can prevent getting sick',\n",
            "       'I feel weird, like a ‘zombie’ on medication',\n",
            "       'Medication makes me feel tired and sluggish',\n",
            "       'Some of your symptoms are made by your mind.', 'You are mentally well',\n",
            "       'You do not need medication', 'Your stay in the hospital is necessary',\n",
            "       'The doctor is right in prescribing medication for you.',\n",
            "       'You do not need to be seen by a doctor or psychiatrist',\n",
            "       'If someone said you have a nervous or mental illness, they would be right',\n",
            "       'None of the unusual things you are experiencing are due to an illness.',\n",
            "       '. Loss of energy or drive', 'Feeling unmotivated or numb',\n",
            "       'Daytime sedation or drowsiness', 'Sleeping too much',\n",
            "       'Muscles being too tense or stiff', 'Muscles trembling or shaking',\n",
            "       'Feeling restless or jittery',\n",
            "       'Need to move around and pace; inability to sit still',\n",
            "       'Trouble getting to sleep or staying asleep (insomnia)',\n",
            "       'Blurry vision', 'Dry mouth', 'Drooling',\n",
            "       'Memory and concentration problems', 'Constipation', 'Weight changes',\n",
            "       'Changes in sexual functioning', 'Menstrual or breast problem',\n",
            "       'I feel over burdened by the number of pills i swallow per day.',\n",
            "       'How often do yo take your drugs',\n",
            "       'How often do you find no medications in the hospital',\n",
            "       'I am satisfied with doctors explanation about mental illness and the need for treatment',\n",
            "       'Do you sometimes stop your medications because of religious or cultural beliefs'],\n",
            "      dtype='object')\n",
            "New class distribution: Counter({1: 55, 0: 55})\n",
            "Shape of X_train_scaled: (110, 59)\n",
            "Shape of y_train_resampled: (110,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Model initialisation"
      ],
      "metadata": {
        "id": "-HMidXW5Yrpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the FeedForwardNN model\n",
        "input_dim = X_train.shape[1]  # number of features in your dataset\n",
        "model = FeedForwardNN(input_dim)\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n"
      ],
      "metadata": {
        "id": "xWcNQjEuYwBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 TRaining the model"
      ],
      "metadata": {
        "id": "0jzHG3RWYyzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YA7eZnOuY_zf",
        "outputId": "744f2211-0d20-4884-c51c-c6892a5ceaac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.6922, Train Accuracy: 0.4909, Val Loss: 0.6916, Val Accuracy: 0.5000\n",
            "Epoch 2/100, Train Loss: 0.6693, Train Accuracy: 0.6273, Val Loss: 0.6799, Val Accuracy: 0.6667\n",
            "Epoch 3/100, Train Loss: 0.6566, Train Accuracy: 0.7273, Val Loss: 0.6689, Val Accuracy: 0.6667\n",
            "Epoch 4/100, Train Loss: 0.6389, Train Accuracy: 0.8545, Val Loss: 0.6591, Val Accuracy: 0.7222\n",
            "Epoch 5/100, Train Loss: 0.6140, Train Accuracy: 0.8636, Val Loss: 0.6487, Val Accuracy: 0.7222\n",
            "Epoch 6/100, Train Loss: 0.5909, Train Accuracy: 0.8636, Val Loss: 0.6374, Val Accuracy: 0.7222\n",
            "Epoch 7/100, Train Loss: 0.5766, Train Accuracy: 0.8818, Val Loss: 0.6254, Val Accuracy: 0.7222\n",
            "Epoch 8/100, Train Loss: 0.5432, Train Accuracy: 0.9000, Val Loss: 0.6130, Val Accuracy: 0.7222\n",
            "Epoch 9/100, Train Loss: 0.5226, Train Accuracy: 0.8909, Val Loss: 0.6015, Val Accuracy: 0.6667\n",
            "Epoch 10/100, Train Loss: 0.4990, Train Accuracy: 0.8818, Val Loss: 0.5928, Val Accuracy: 0.6667\n",
            "Epoch 11/100, Train Loss: 0.4689, Train Accuracy: 0.8818, Val Loss: 0.5854, Val Accuracy: 0.6667\n",
            "Epoch 12/100, Train Loss: 0.4202, Train Accuracy: 0.8909, Val Loss: 0.5824, Val Accuracy: 0.6667\n",
            "Epoch 13/100, Train Loss: 0.3928, Train Accuracy: 0.8909, Val Loss: 0.5815, Val Accuracy: 0.6667\n",
            "Epoch 14/100, Train Loss: 0.3556, Train Accuracy: 0.9091, Val Loss: 0.5845, Val Accuracy: 0.6667\n",
            "Epoch 15/100, Train Loss: 0.3398, Train Accuracy: 0.9182, Val Loss: 0.5896, Val Accuracy: 0.6667\n",
            "Epoch 16/100, Train Loss: 0.3001, Train Accuracy: 0.9364, Val Loss: 0.5952, Val Accuracy: 0.6667\n",
            "Epoch 17/100, Train Loss: 0.2542, Train Accuracy: 0.9455, Val Loss: 0.6012, Val Accuracy: 0.6667\n",
            "Epoch 18/100, Train Loss: 0.2254, Train Accuracy: 0.9545, Val Loss: 0.6076, Val Accuracy: 0.6667\n",
            "Epoch 19/100, Train Loss: 0.1941, Train Accuracy: 0.9818, Val Loss: 0.6174, Val Accuracy: 0.6667\n",
            "Epoch 20/100, Train Loss: 0.1741, Train Accuracy: 0.9909, Val Loss: 0.6287, Val Accuracy: 0.6667\n",
            "Epoch 21/100, Train Loss: 0.1448, Train Accuracy: 0.9909, Val Loss: 0.6403, Val Accuracy: 0.7778\n",
            "Epoch 22/100, Train Loss: 0.1428, Train Accuracy: 0.9909, Val Loss: 0.6542, Val Accuracy: 0.7778\n",
            "Epoch 23/100, Train Loss: 0.1183, Train Accuracy: 0.9909, Val Loss: 0.6695, Val Accuracy: 0.7778\n",
            "Epoch 24/100, Train Loss: 0.0948, Train Accuracy: 0.9909, Val Loss: 0.6826, Val Accuracy: 0.7778\n",
            "Epoch 25/100, Train Loss: 0.0754, Train Accuracy: 0.9909, Val Loss: 0.6980, Val Accuracy: 0.7778\n",
            "Epoch 26/100, Train Loss: 0.0710, Train Accuracy: 1.0000, Val Loss: 0.7145, Val Accuracy: 0.7778\n",
            "Epoch 27/100, Train Loss: 0.0617, Train Accuracy: 1.0000, Val Loss: 0.7320, Val Accuracy: 0.7778\n",
            "Epoch 28/100, Train Loss: 0.0487, Train Accuracy: 1.0000, Val Loss: 0.7493, Val Accuracy: 0.7778\n",
            "Epoch 29/100, Train Loss: 0.0410, Train Accuracy: 1.0000, Val Loss: 0.7660, Val Accuracy: 0.8333\n",
            "Epoch 30/100, Train Loss: 0.0372, Train Accuracy: 1.0000, Val Loss: 0.7831, Val Accuracy: 0.8333\n",
            "Epoch 31/100, Train Loss: 0.0327, Train Accuracy: 1.0000, Val Loss: 0.7993, Val Accuracy: 0.8333\n",
            "Epoch 32/100, Train Loss: 0.0274, Train Accuracy: 1.0000, Val Loss: 0.8142, Val Accuracy: 0.8333\n",
            "Epoch 33/100, Train Loss: 0.0229, Train Accuracy: 1.0000, Val Loss: 0.8289, Val Accuracy: 0.8333\n",
            "Epoch 34/100, Train Loss: 0.0200, Train Accuracy: 1.0000, Val Loss: 0.8437, Val Accuracy: 0.8333\n",
            "Epoch 35/100, Train Loss: 0.0170, Train Accuracy: 1.0000, Val Loss: 0.8571, Val Accuracy: 0.8333\n",
            "Epoch 36/100, Train Loss: 0.0167, Train Accuracy: 1.0000, Val Loss: 0.8707, Val Accuracy: 0.8333\n",
            "Epoch 37/100, Train Loss: 0.0146, Train Accuracy: 1.0000, Val Loss: 0.8840, Val Accuracy: 0.8333\n",
            "Epoch 38/100, Train Loss: 0.0121, Train Accuracy: 1.0000, Val Loss: 0.8959, Val Accuracy: 0.8333\n",
            "Epoch 39/100, Train Loss: 0.0112, Train Accuracy: 1.0000, Val Loss: 0.9075, Val Accuracy: 0.8333\n",
            "Epoch 40/100, Train Loss: 0.0101, Train Accuracy: 1.0000, Val Loss: 0.9187, Val Accuracy: 0.8333\n",
            "Epoch 41/100, Train Loss: 0.0104, Train Accuracy: 1.0000, Val Loss: 0.9297, Val Accuracy: 0.8333\n",
            "Epoch 42/100, Train Loss: 0.0089, Train Accuracy: 1.0000, Val Loss: 0.9405, Val Accuracy: 0.8333\n",
            "Epoch 43/100, Train Loss: 0.0089, Train Accuracy: 1.0000, Val Loss: 0.9510, Val Accuracy: 0.8333\n",
            "Epoch 44/100, Train Loss: 0.0071, Train Accuracy: 1.0000, Val Loss: 0.9606, Val Accuracy: 0.8333\n",
            "Epoch 45/100, Train Loss: 0.0068, Train Accuracy: 1.0000, Val Loss: 0.9700, Val Accuracy: 0.8333\n",
            "Epoch 46/100, Train Loss: 0.0061, Train Accuracy: 1.0000, Val Loss: 0.9785, Val Accuracy: 0.8333\n",
            "Epoch 47/100, Train Loss: 0.0061, Train Accuracy: 1.0000, Val Loss: 0.9867, Val Accuracy: 0.8333\n",
            "Epoch 48/100, Train Loss: 0.0058, Train Accuracy: 1.0000, Val Loss: 0.9948, Val Accuracy: 0.8333\n",
            "Epoch 49/100, Train Loss: 0.0057, Train Accuracy: 1.0000, Val Loss: 1.0034, Val Accuracy: 0.8333\n",
            "Epoch 50/100, Train Loss: 0.0052, Train Accuracy: 1.0000, Val Loss: 1.0108, Val Accuracy: 0.8333\n",
            "Epoch 51/100, Train Loss: 0.0049, Train Accuracy: 1.0000, Val Loss: 1.0179, Val Accuracy: 0.8333\n",
            "Epoch 52/100, Train Loss: 0.0046, Train Accuracy: 1.0000, Val Loss: 1.0249, Val Accuracy: 0.8333\n",
            "Epoch 53/100, Train Loss: 0.0046, Train Accuracy: 1.0000, Val Loss: 1.0318, Val Accuracy: 0.8333\n",
            "Epoch 54/100, Train Loss: 0.0041, Train Accuracy: 1.0000, Val Loss: 1.0387, Val Accuracy: 0.8333\n",
            "Epoch 55/100, Train Loss: 0.0038, Train Accuracy: 1.0000, Val Loss: 1.0458, Val Accuracy: 0.8333\n",
            "Epoch 56/100, Train Loss: 0.0037, Train Accuracy: 1.0000, Val Loss: 1.0526, Val Accuracy: 0.8333\n",
            "Epoch 57/100, Train Loss: 0.0037, Train Accuracy: 1.0000, Val Loss: 1.0593, Val Accuracy: 0.8333\n",
            "Epoch 58/100, Train Loss: 0.0033, Train Accuracy: 1.0000, Val Loss: 1.0664, Val Accuracy: 0.8333\n",
            "Epoch 59/100, Train Loss: 0.0035, Train Accuracy: 1.0000, Val Loss: 1.0728, Val Accuracy: 0.8333\n",
            "Epoch 60/100, Train Loss: 0.0031, Train Accuracy: 1.0000, Val Loss: 1.0788, Val Accuracy: 0.8333\n",
            "Epoch 61/100, Train Loss: 0.0031, Train Accuracy: 1.0000, Val Loss: 1.0847, Val Accuracy: 0.8333\n",
            "Epoch 62/100, Train Loss: 0.0029, Train Accuracy: 1.0000, Val Loss: 1.0904, Val Accuracy: 0.8333\n",
            "Epoch 63/100, Train Loss: 0.0028, Train Accuracy: 1.0000, Val Loss: 1.0958, Val Accuracy: 0.8333\n",
            "Epoch 64/100, Train Loss: 0.0025, Train Accuracy: 1.0000, Val Loss: 1.1013, Val Accuracy: 0.8333\n",
            "Epoch 65/100, Train Loss: 0.0027, Train Accuracy: 1.0000, Val Loss: 1.1067, Val Accuracy: 0.8333\n",
            "Epoch 66/100, Train Loss: 0.0024, Train Accuracy: 1.0000, Val Loss: 1.1125, Val Accuracy: 0.8333\n",
            "Epoch 67/100, Train Loss: 0.0024, Train Accuracy: 1.0000, Val Loss: 1.1179, Val Accuracy: 0.8333\n",
            "Epoch 68/100, Train Loss: 0.0023, Train Accuracy: 1.0000, Val Loss: 1.1229, Val Accuracy: 0.8333\n",
            "Epoch 69/100, Train Loss: 0.0021, Train Accuracy: 1.0000, Val Loss: 1.1282, Val Accuracy: 0.8333\n",
            "Epoch 70/100, Train Loss: 0.0022, Train Accuracy: 1.0000, Val Loss: 1.1332, Val Accuracy: 0.8333\n",
            "Epoch 71/100, Train Loss: 0.0020, Train Accuracy: 1.0000, Val Loss: 1.1383, Val Accuracy: 0.8333\n",
            "Epoch 72/100, Train Loss: 0.0020, Train Accuracy: 1.0000, Val Loss: 1.1433, Val Accuracy: 0.8333\n",
            "Epoch 73/100, Train Loss: 0.0020, Train Accuracy: 1.0000, Val Loss: 1.1481, Val Accuracy: 0.8333\n",
            "Epoch 74/100, Train Loss: 0.0018, Train Accuracy: 1.0000, Val Loss: 1.1531, Val Accuracy: 0.8333\n",
            "Epoch 75/100, Train Loss: 0.0018, Train Accuracy: 1.0000, Val Loss: 1.1577, Val Accuracy: 0.8333\n",
            "Epoch 76/100, Train Loss: 0.0016, Train Accuracy: 1.0000, Val Loss: 1.1623, Val Accuracy: 0.8333\n",
            "Epoch 77/100, Train Loss: 0.0019, Train Accuracy: 1.0000, Val Loss: 1.1668, Val Accuracy: 0.8333\n",
            "Epoch 78/100, Train Loss: 0.0016, Train Accuracy: 1.0000, Val Loss: 1.1713, Val Accuracy: 0.8333\n",
            "Epoch 79/100, Train Loss: 0.0017, Train Accuracy: 1.0000, Val Loss: 1.1760, Val Accuracy: 0.8333\n",
            "Epoch 80/100, Train Loss: 0.0015, Train Accuracy: 1.0000, Val Loss: 1.1804, Val Accuracy: 0.8333\n",
            "Epoch 81/100, Train Loss: 0.0015, Train Accuracy: 1.0000, Val Loss: 1.1845, Val Accuracy: 0.8333\n",
            "Epoch 82/100, Train Loss: 0.0014, Train Accuracy: 1.0000, Val Loss: 1.1886, Val Accuracy: 0.8333\n",
            "Epoch 83/100, Train Loss: 0.0016, Train Accuracy: 1.0000, Val Loss: 1.1925, Val Accuracy: 0.8333\n",
            "Epoch 84/100, Train Loss: 0.0013, Train Accuracy: 1.0000, Val Loss: 1.1963, Val Accuracy: 0.8333\n",
            "Epoch 85/100, Train Loss: 0.0014, Train Accuracy: 1.0000, Val Loss: 1.2002, Val Accuracy: 0.8333\n",
            "Epoch 86/100, Train Loss: 0.0013, Train Accuracy: 1.0000, Val Loss: 1.2040, Val Accuracy: 0.8333\n",
            "Epoch 87/100, Train Loss: 0.0013, Train Accuracy: 1.0000, Val Loss: 1.2077, Val Accuracy: 0.8333\n",
            "Epoch 88/100, Train Loss: 0.0012, Train Accuracy: 1.0000, Val Loss: 1.2116, Val Accuracy: 0.8333\n",
            "Epoch 89/100, Train Loss: 0.0012, Train Accuracy: 1.0000, Val Loss: 1.2153, Val Accuracy: 0.8333\n",
            "Epoch 90/100, Train Loss: 0.0012, Train Accuracy: 1.0000, Val Loss: 1.2190, Val Accuracy: 0.8333\n",
            "Epoch 91/100, Train Loss: 0.0012, Train Accuracy: 1.0000, Val Loss: 1.2230, Val Accuracy: 0.8333\n",
            "Epoch 92/100, Train Loss: 0.0011, Train Accuracy: 1.0000, Val Loss: 1.2270, Val Accuracy: 0.8333\n",
            "Epoch 93/100, Train Loss: 0.0011, Train Accuracy: 1.0000, Val Loss: 1.2306, Val Accuracy: 0.8333\n",
            "Epoch 94/100, Train Loss: 0.0011, Train Accuracy: 1.0000, Val Loss: 1.2344, Val Accuracy: 0.8333\n",
            "Epoch 95/100, Train Loss: 0.0010, Train Accuracy: 1.0000, Val Loss: 1.2383, Val Accuracy: 0.8333\n",
            "Epoch 96/100, Train Loss: 0.0010, Train Accuracy: 1.0000, Val Loss: 1.2420, Val Accuracy: 0.8333\n",
            "Epoch 97/100, Train Loss: 0.0010, Train Accuracy: 1.0000, Val Loss: 1.2455, Val Accuracy: 0.8333\n",
            "Epoch 98/100, Train Loss: 0.0010, Train Accuracy: 1.0000, Val Loss: 1.2489, Val Accuracy: 0.8333\n",
            "Epoch 99/100, Train Loss: 0.0009, Train Accuracy: 1.0000, Val Loss: 1.2521, Val Accuracy: 0.8333\n",
            "Epoch 100/100, Train Loss: 0.0010, Train Accuracy: 1.0000, Val Loss: 1.2556, Val Accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving best model"
      ],
      "metadata": {
        "id": "euqsffLGZVVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model for inference\n",
        "best_model = FeedForwardNN(input_dim)\n",
        "best_model.load_state_dict(torch.load(\"best_feedforward_model.pth\"))\n",
        "best_model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvj6IrhQZXf7",
        "outputId": "ee7d0fb9-68fe-45d3-a8ef-f72f17022784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-143-55df29be72a9>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model.load_state_dict(torch.load(\"best_feedforward_model.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedForwardNN(\n",
              "  (fc1): Linear(in_features=59, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "N1AXofKU7B3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ff_model = FeedForwardNN(input_dim)  # Redefine model\n",
        "ff_model.load_state_dict(torch.load(\"best_feedforward_model.pth\"))  # Load weights\n",
        "ff_model.to(device)\n",
        "ff_model.eval()  # Set to evaluation mode\n"
      ],
      "metadata": {
        "id": "SQkaRBYY38EH",
        "outputId": "86232269-df49-4bbd-91d5-de984b7b0f3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-184-f0313323dbfc>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ff_model.load_state_dict(torch.load(\"best_feedforward_model.pth\"))  # Load weights\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedForwardNN(\n",
              "  (fc1): Linear(in_features=59, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
              "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "ff_model.eval()\n",
        "\n",
        "# Start timing evaluation\n",
        "start_time = time.time()\n",
        "\n",
        "# Get predictions\n",
        "y_prob_ffnn = ff_model(X_test_tensor).detach().cpu().numpy().flatten()  # Ensure proper shape\n",
        "\n",
        "# Convert probabilities to binary predictions using a 0.5 threshold\n",
        "y_pred_ffnn = (y_prob_ffnn >= 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "ffnn_precision = precision_score(y_test, y_pred_ffnn)\n",
        "ffnn_recall = recall_score(y_test, y_pred_ffnn)\n",
        "ffnn_f1 = f1_score(y_test, y_pred_ffnn)\n",
        "ffnn_auc = roc_auc_score(y_test, y_prob_ffnn)  # Use probabilities for AUC\n",
        "\n",
        "# Stop timing evaluation\n",
        "ffnn_time = time.time() - start_time\n",
        "\n",
        "# Print results\n",
        "print(f\"FastForward Neural Network Evaluation:\")\n",
        "print(f\"Precision: {ffnn_precision:.4f} | Recall: {ffnn_recall:.4f}\")\n",
        "print(f\"F1-Score: {ffnn_f1:.4f} | AUC: {ffnn_auc:.4f}\")\n",
        "print(f\"Evaluation Time: {ffnn_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "9Zzd6CcO4BDq",
        "outputId": "21e38b11-f227-4862-a8d6-5641bdc64698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastForward Neural Network Evaluation:\n",
            "Precision: 0.1667 | Recall: 0.1667\n",
            "F1-Score: 0.1667 | AUC: 0.4028\n",
            "Evaluation Time: 0.01 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Logisitic Regretion"
      ],
      "metadata": {
        "id": "KlciCDP2a9ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Import Libraries"
      ],
      "metadata": {
        "id": "UbYCxxEFcj16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming data preprocessing has been done and the data is available in these variables\n",
        "# X_train, X_val, X_test, y_train, y_val, y_test\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "xV1CAogFbJLk"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Training"
      ],
      "metadata": {
        "id": "xN9ha_HTcv_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Logistic Regression model\n",
        "logreg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "logreg_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = end_time - start_time\n"
      ],
      "metadata": {
        "id": "oZZWnBw6c12n"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Evaluation"
      ],
      "metadata": {
        "id": "deejg0w_c3h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_logreg = logreg_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_logreg = (y_prob_logreg >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "logreg_precision = precision_score(y_test, y_pred_logreg)\n",
        "logreg_recall = recall_score(y_test, y_pred_logreg)\n",
        "logreg_f1 = f1_score(y_test, y_pred_logreg)\n",
        "logreg_auc = roc_auc_score(y_test, y_prob_logreg)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "model_results = {\n",
        "    \"Model\": [\"Logistic Regression\"],\n",
        "    \"Precision\": [logreg_precision],\n",
        "    \"Recall\": [logreg_recall],\n",
        "    \"F1-Score\": [logreg_f1],\n",
        "    \"AUC\": [logreg_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(model_results)\n",
        "\n",
        "# Display results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "vwXwqxDM_XKy",
        "outputId": "3556a0ed-01e4-4849-ccb0-07a3b370d536",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0  Logistic Regression       0.25  0.166667       0.2  0.347222   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.015465  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Support Vector Machine"
      ],
      "metadata": {
        "id": "RYj2KN_odCZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming data preprocessing has been done and the data is available in these variables\n",
        "# X_train, X_val, X_test, y_train, y_val, y_test\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Initialize the SVM model with a radial basis function kernel\n",
        "svm_model = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = end_time - start_time\n"
      ],
      "metadata": {
        "id": "rweuNg7udRko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "Z9kyZ_mNde_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities\n",
        "y_prob_svm = svm_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_svm = (y_prob_svm >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "svm_precision = precision_score(y_test, y_pred_svm)\n",
        "svm_recall = recall_score(y_test, y_pred_svm)\n",
        "svm_f1 = f1_score(y_test, y_pred_svm)\n",
        "svm_auc = roc_auc_score(y_test, y_prob_svm)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "svm_results = {\n",
        "    \"Model\": [\"Support Vector Machine (SVM)\"],\n",
        "    \"Precision\": [svm_precision],\n",
        "    \"Recall\": [svm_recall],\n",
        "    \"F1-Score\": [svm_f1],\n",
        "    \"AUC\": [svm_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "svm_results_df = pd.DataFrame(svm_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, svm_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "XX1refb4_2Td",
        "outputId": "b85c86da-d8ef-469a-9924-1618cc618348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression       0.25  0.166667       0.2  0.347222   \n",
            "1  Support Vector Machine (SVM)       0.00  0.000000       0.0  0.416667   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.015465  \n",
            "1           0.015465  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Random Forest Classifier"
      ],
      "metadata": {
        "id": "Gxv7fW3_dqK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming data preprocessing has been done and the data is available in these variables\n",
        "# X_train, X_val, X_test, y_train, y_val, y_test\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#TRAINING\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = end_time - start_time\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_6Sq7pKDd_kO"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "PK6DKYoIAtWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities\n",
        "y_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_rf = (y_prob_rf >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "rf_precision = precision_score(y_test, y_pred_rf)\n",
        "rf_recall = recall_score(y_test, y_pred_rf)\n",
        "rf_f1 = f1_score(y_test, y_pred_rf)\n",
        "rf_auc = roc_auc_score(y_test, y_prob_rf)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "rf_results = {\n",
        "    \"Model\": [\"Random Forest\"],\n",
        "    \"Precision\": [rf_precision],\n",
        "    \"Recall\": [rf_recall],\n",
        "    \"F1-Score\": [rf_f1],\n",
        "    \"AUC\": [rf_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "rf_results_df = pd.DataFrame(rf_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, rf_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "wAyJ1TijAmpX",
        "outputId": "0eaf49ae-5206-4125-c7a9-23eef22725cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression       0.25  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)       0.00  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest       1.00  0.166667  0.285714  0.465278   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.015465  \n",
            "1           0.015465  \n",
            "2           0.015465  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a K-Nearest Neighbor"
      ],
      "metadata": {
        "id": "VC7wLDMjfDAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import time\n",
        "\n",
        "# Initialize KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate training time\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_knn = knn_model.predict(X_test_scaled)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "89j6P0lBfPE2"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities\n",
        "y_prob_knn = knn_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_knn = (y_prob_knn >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "knn_precision = precision_score(y_test, y_pred_knn)\n",
        "knn_recall = recall_score(y_test, y_pred_knn)\n",
        "knn_f1 = f1_score(y_test, y_pred_knn)\n",
        "knn_auc = roc_auc_score(y_test, y_prob_knn)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "knn_results = {\n",
        "    \"Model\": [\"K-Nearest Neighbors (KNN)\"],\n",
        "    \"Precision\": [knn_precision],\n",
        "    \"Recall\": [knn_recall],\n",
        "    \"F1-Score\": [knn_f1],\n",
        "    \"AUC\": [knn_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "knn_results_df = pd.DataFrame(knn_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, knn_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "KHl2ICDqBh_n",
        "outputId": "a740c932-0358-48c5-f454-ef78881cf73c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.015465  \n",
            "1           0.015465  \n",
            "2           0.015465  \n",
            "3           0.239112  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Gradient Boost machine"
      ],
      "metadata": {
        "id": "YEVJvZG7fUYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Initialize Gradient Boosting Classifier\n",
        "gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "gbm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on the validation and test sets\n",
        "y_val_pred = gbm_model.predict(X_val_scaled)\n",
        "y_test_pred = gbm_model.predict(X_test_scaled)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_knn = knn_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_knn = (y_prob_knn >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "knn_precision = precision_score(y_test, y_pred_knn)\n",
        "knn_recall = recall_score(y_test, y_pred_knn)\n",
        "knn_f1 = f1_score(y_test, y_pred_knn)\n",
        "knn_auc = roc_auc_score(y_test, y_prob_knn)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "knn_results = {\n",
        "    \"Model\": [\"K-Nearest Neighbors (KNN)\"],\n",
        "    \"Precision\": [knn_precision],\n",
        "    \"Recall\": [knn_recall],\n",
        "    \"F1-Score\": [knn_f1],\n",
        "    \"AUC\": [knn_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "knn_results_df = pd.DataFrame(knn_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, knn_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnfUNlupf0Vn",
        "outputId": "46d15c03-85eb-49a4-c564-4c6d850baefb"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "4     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.015465  \n",
            "1           0.015465  \n",
            "2           0.015465  \n",
            "3           0.239112  \n",
            "4           0.253225  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing XGBoost"
      ],
      "metadata": {
        "id": "ODQ1xV3bf3dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlYAPJOsgTrO",
        "outputId": "b89e228f-dbd8-4708-83a8-f969dcac4f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgboost_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "xgboost_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on the validation and test sets\n",
        "y_val_pred = xgboost_model.predict(X_val_scaled)\n",
        "y_test_pred = xgboost_model.predict(X_test_scaled)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_xgb = xgboost_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_xgb = (y_prob_xgb >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "xgb_precision = precision_score(y_test, y_pred_xgb)\n",
        "xgb_recall = recall_score(y_test, y_pred_xgb)\n",
        "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
        "xgb_auc = roc_auc_score(y_test, y_prob_xgb)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "xgb_results = {\n",
        "    \"Model\": [\"XGBoost\"],\n",
        "    \"Precision\": [xgb_precision],\n",
        "    \"Recall\": [xgb_recall],\n",
        "    \"F1-Score\": [xgb_f1],\n",
        "    \"AUC\": [xgb_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "xgb_results_df = pd.DataFrame(xgb_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, xgb_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRRCjGpcgcEf",
        "outputId": "17844e8b-9402-4737-a6b0-5ca06ea695fe"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "4     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "5                       XGBoost   0.200000  0.166667  0.181818  0.375000   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.015465  \n",
            "1           0.015465  \n",
            "2           0.015465  \n",
            "3           0.239112  \n",
            "4           0.253225  \n",
            "5           0.066676  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing LightGBM"
      ],
      "metadata": {
        "id": "trCnfvrqhGK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n"
      ],
      "metadata": {
        "id": "RrGTq8YBiqyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Initialize LightGBM Classifier\n",
        "lgbm_model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "lgbm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on the validation and test sets\n",
        "y_val_pred = lgbm_model.predict(X_val_scaled)\n",
        "y_test_pred = lgbm_model.predict(X_test_scaled)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_lgbm = lgbm_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_lgbm = (y_prob_lgbm >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "lgbm_precision = precision_score(y_test, y_pred_lgbm)\n",
        "lgbm_recall = recall_score(y_test, y_pred_lgbm)\n",
        "lgbm_f1 = f1_score(y_test, y_pred_lgbm)\n",
        "lgbm_auc = roc_auc_score(y_test, y_prob_lgbm)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "lgbm_results = {\n",
        "    \"Model\": [\"LightGBM\"],\n",
        "    \"Precision\": [lgbm_precision],\n",
        "    \"Recall\": [lgbm_recall],\n",
        "    \"F1-Score\": [lgbm_f1],\n",
        "    \"AUC\": [lgbm_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "lgbm_results_df = pd.DataFrame(lgbm_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, lgbm_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "cc2tl1xiisY2",
        "outputId": "c47a48b5-7dbc-41b4-efc4-1168e2996d32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 26, number of negative: 55\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000201 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 217\n",
            "[LightGBM] [Info] Number of data points in the train set: 81, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.320988 -> initscore=-0.749237\n",
            "[LightGBM] [Info] Start training from score -0.749237\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "4     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "5                       XGBoost   0.200000  0.166667  0.181818  0.375000   \n",
            "6                      LightGBM   0.250000  0.166667  0.200000  0.458333   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.015465  \n",
            "1           0.015465  \n",
            "2           0.015465  \n",
            "3           0.239112  \n",
            "4           0.253225  \n",
            "5           0.066676  \n",
            "6           0.183859  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing CATBoost"
      ],
      "metadata": {
        "id": "kWUT3-V_i3jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n"
      ],
      "metadata": {
        "id": "bYGZZJ5YjQqR",
        "outputId": "f5f1bcf5-247b-4ec2-dae1-41cb4c346c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.13.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, random_seed=42, verbose=0)\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "catboost_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record end time\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on validation and test sets\n",
        "y_val_pred = catboost_model.predict(X_val_scaled)\n",
        "y_test_pred = catboost_model.predict(X_test_scaled)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_catboost = catboost_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_catboost = (y_prob_catboost >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "catboost_precision = precision_score(y_test, y_pred_catboost)\n",
        "catboost_recall = recall_score(y_test, y_pred_catboost)\n",
        "catboost_f1 = f1_score(y_test, y_pred_catboost)\n",
        "catboost_auc = roc_auc_score(y_test, y_prob_catboost)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "catboost_results = {\n",
        "    \"Model\": [\"CatBoost\"],\n",
        "    \"Precision\": [catboost_precision],\n",
        "    \"Recall\": [catboost_recall],\n",
        "    \"F1-Score\": [catboost_f1],\n",
        "    \"AUC\": [catboost_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "catboost_results_df = pd.DataFrame(catboost_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, catboost_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "A81YYFTgjVP-",
        "outputId": "411f4bfe-d65c-4bc0-b045-ab7bc1837d05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "4     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "5                       XGBoost   0.200000  0.166667  0.181818  0.375000   \n",
            "6                      LightGBM   0.250000  0.166667  0.200000  0.458333   \n",
            "7                      CatBoost   0.250000  0.166667  0.200000  0.416667   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.015465  \n",
            "1           0.015465  \n",
            "2           0.015465  \n",
            "3           0.239112  \n",
            "4           0.253225  \n",
            "5           0.066676  \n",
            "6           0.183859  \n",
            "7           0.235054  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing A neural network"
      ],
      "metadata": {
        "id": "Pv6CDU82mFzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Define the Neural Network Model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.output = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.output(x))\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X_train_scaled.shape[1]  # Number of features\n",
        "model = NeuralNetwork(input_dim)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Prepare data (assuming X_train_scaled, y_train are numpy arrays or tensors)\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "# Initialize the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "start_time = time.time()\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "for epoch in range(100):  # Training for 100 epochs\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "\n",
        "# Make predictions on the validation and test sets\n",
        "with torch.no_grad():\n",
        "    y_val_pred_prob = model(X_val_tensor).squeeze().cpu().numpy()\n",
        "    y_test_pred_prob = model(X_test_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_val_pred = (y_val_pred_prob >= 0.5).astype(int)\n",
        "y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics for validation set\n",
        "val_precision = precision_score(y_val, y_val_pred)\n",
        "val_recall = recall_score(y_val, y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "val_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
        "\n",
        "# Calculate evaluation metrics for test set\n",
        "test_precision = precision_score(y_test, y_test_pred)\n",
        "test_recall = recall_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Neural Network Model Evaluation (Validation Set):\")\n",
        "print(f\"Validation Precision: {val_precision:.4f} | Validation Recall: {val_recall:.4f}\")\n",
        "print(f\"Validation F1-Score: {val_f1:.4f} | Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nNeural Network Model Evaluation (Test Set):\")\n",
        "print(f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f} | Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Time: {training_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "VKJXJE1UDn3t",
        "outputId": "ea46eeef-fbf1-43e2-cd4d-68e321a6a530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network Model Evaluation (Validation Set):\n",
            "Validation Precision: 0.7500 | Validation Recall: 0.5000\n",
            "Validation F1-Score: 0.6000 | Validation AUC: 0.7361\n",
            "\n",
            "Neural Network Model Evaluation (Test Set):\n",
            "Test Precision: 0.2500 | Test Recall: 0.1667\n",
            "Test F1-Score: 0.2000 | Test AUC: 0.3889\n",
            "\n",
            "Training Time: 0.3942 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing DNN"
      ],
      "metadata": {
        "id": "YXYQSVqtvFPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Define the Deep Neural Network (DNN) Model\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # First hidden layer with 128 units\n",
        "        self.fc2 = nn.Linear(128, 64)         # Second hidden layer with 64 units\n",
        "        self.fc3 = nn.Linear(64, 32)          # Third hidden layer with 32 units\n",
        "        self.fc4 = nn.Linear(32, 1)           # Output layer with 1 unit (for binary classification)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))  # ReLU activation after first layer\n",
        "        x = self.relu(self.fc2(x))  # ReLU activation after second layer\n",
        "        x = self.relu(self.fc3(x))  # ReLU activation after third layer\n",
        "        x = self.sigmoid(self.fc4(x))  # Sigmoid activation for binary classification\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X_train_scaled.shape[1]  # Number of features (input dimension)\n",
        "model = DNN(input_dim)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Prepare data (convert to tensors)\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "# Initialize loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
        "\n",
        "# Training the model\n",
        "start_time = time.time()\n",
        "\n",
        "# Set model to training mode\n",
        "model.train()\n",
        "for epoch in range(100):  # Training for 100 epochs\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    outputs = model(X_train_tensor)  # Forward pass\n",
        "    loss = criterion(outputs, y_train_tensor)  # Calculate the loss\n",
        "    loss.backward()  # Backpropagate the loss\n",
        "    optimizer.step()  # Update the model parameters\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Evaluation\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Make predictions on validation and test sets\n",
        "with torch.no_grad():  # No gradients needed for evaluation\n",
        "    y_val_pred_prob = model(X_val_tensor).squeeze().cpu().numpy()  # Validation predictions\n",
        "    y_test_pred_prob = model(X_test_tensor).squeeze().cpu().numpy()  # Test predictions\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_val_pred = (y_val_pred_prob >= 0.5).astype(int)\n",
        "y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics for validation set\n",
        "val_precision = precision_score(y_val, y_val_pred)\n",
        "val_recall = recall_score(y_val, y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "val_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
        "\n",
        "# Calculate evaluation metrics for test set\n",
        "test_precision = precision_score(y_test, y_test_pred)\n",
        "test_recall = recall_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"DNN Model Evaluation (Validation Set):\")\n",
        "print(f\"Validation Precision: {val_precision:.4f} | Validation Recall: {val_recall:.4f}\")\n",
        "print(f\"Validation F1-Score: {val_f1:.4f} | Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nDNN Model Evaluation (Test Set):\")\n",
        "print(f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f} | Test AUC: {test_auc:.4f}\")\n",
        "\n",
        "print(f\"\\nTraining Time: {training_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "1z7s2v-EFNBz",
        "outputId": "efc0a9af-534d-4bff-df10-319716741189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNN Model Evaluation (Validation Set):\n",
            "Validation Precision: 1.0000 | Validation Recall: 0.5000\n",
            "Validation F1-Score: 0.6667 | Validation AUC: 0.7778\n",
            "\n",
            "DNN Model Evaluation (Test Set):\n",
            "Test Precision: 0.2500 | Test Recall: 0.1667\n",
            "Test F1-Score: 0.2000 | Test AUC: 0.3472\n",
            "\n",
            "Training Time: 0.3088 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing The Models"
      ],
      "metadata": {
        "id": "gRKUKYlMwVEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the following variables have been defined after evaluating each model:\n",
        "# log_precision, rf_precision, svm_precision, xgb_precision, lgbm_precision, catboost_precision,\n",
        "# knn_precision, dnn_precision, and so on for recall, F1-Score, AUC, and training time.\n",
        "\n",
        "model_comparison = pd.DataFrame({\n",
        "    \"Model\": [\n",
        "        \"Logistic Regression\", \"Random Forest\", \"Support Vector Machine\",\n",
        "        \"XGBoost\", \"LightGBM\", \"CatBoost\", \"K-Nearest Neighbors\", \"Deep Neural Network\"\n",
        "    ],\n",
        "    \"Precision\": [lr_precision, rf_precision, svm_precision,\n",
        "                  xgb_precision, lgbm_precision, catboost_precision,\n",
        "                  knn_precision, dnn_precision],\n",
        "    \"Recall\": [lr_recall, rf_recall, svm_recall,\n",
        "               xgb_recall, lgbm_recall, catboost_recall,\n",
        "               knn_recall, dnn_recall],\n",
        "    \"F1-Score\": [lr_f1, rf_f1, svm_f1,\n",
        "                 xgb_f1, lgbm_f1, catboost_f1,\n",
        "                 knn_f1, dnn_f1],\n",
        "    \"AUC\": [lr_auc, rf_auc, svm_auc,\n",
        "            xgb_auc, lgbm_auc, catboost_auc,\n",
        "            knn_auc, dnn_auc],\n",
        "    \"Training Time (s)\": [lr_time, rf_time, svm_time,\n",
        "                          xgb_time, lgbm_time, catboost_time,\n",
        "                          knn_time, dnn_time],\n",
        "    \"Learning Rate\": [0.001, \"N/A\", \"N/A\", 0.1, 0.05, 0.1, \"N/A\", 0.001]\n",
        "})\n",
        "\n",
        "# Sort models by highest AUC\n",
        "model_comparison = model_comparison.sort_values(by=\"AUC\", ascending=False)\n",
        "\n",
        "# Display the table\n",
        "print(model_comparison)\n"
      ],
      "metadata": {
        "id": "jHFuWt6PF9kk",
        "outputId": "4555268c-adf5-499c-bed6-5069411c59a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lr_precision' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-209-4ebc588c088a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"XGBoost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LightGBM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CatBoost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"K-Nearest Neighbors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Deep Neural Network\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     ],\n\u001b[0;32m---> 12\u001b[0;31m     \"Precision\": [lr_precision, rf_precision, svm_precision,\n\u001b[0m\u001b[1;32m     13\u001b[0m                   \u001b[0mxgb_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgbm_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatboost_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                   knn_precision, dnn_precision],\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lr_precision' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Store the results of all models\n",
        "model_comparison = pd.DataFrame({\n",
        "    \"Model\": [\n",
        "        \"Logistic Regression\", \"Random Forest\", \"Support Vector Machine\",\n",
        "        \"XGBoost\", \"Gradient Boosting\", \"LightGBM\", \"Naive Bayes\",\n",
        "        \"K-Nearest Neighbors\", \"Decision Tree\", \"Deep Neural Network\"\n",
        "    ],\n",
        "    \"Precision\": [log_precision, rf_precision, svm_precision,\n",
        "                  xgb_precision, gbm_precision, lgbm_precision,\n",
        "                  nb_precision, knn_precision, dt_precision, dnn_precision],\n",
        "    \"Recall\": [log_recall, rf_recall, svm_recall,\n",
        "               xgb_recall, gbm_recall, lgbm_recall,\n",
        "               nb_recall, knn_recall, dt_recall, dnn_recall],\n",
        "    \"F1-Score\": [log_f1, rf_f1, svm_f1,\n",
        "                 xgb_f1, gbm_f1, lgbm_f1,\n",
        "                 nb_f1, knn_f1, dt_f1, dnn_f1],\n",
        "    \"AUC\": [log_auc, rf_auc, svm_auc,\n",
        "            xgb_auc, gbm_auc, lgbm_auc,\n",
        "            nb_auc, knn_auc, dt_auc, dnn_auc],\n",
        "    \"Training Time (s)\": [log_time, rf_time, svm_time,\n",
        "                          xgb_time, gbm_time, lgbm_time,\n",
        "                          nb_time, knn_time, dt_time, dnn_time],\n",
        "    \"Learning Rate\": [0.001, \"N/A\", \"N/A\", 0.1, 0.1, 0.05, \"N/A\", \"N/A\", \"N/A\", 0.001]\n",
        "})\n",
        "\n",
        "# Sort models by highest AUC\n",
        "model_comparison = model_comparison.sort_values(by=\"AUC\", ascending=False)\n",
        "\n",
        "# Display the table\n",
        "print(model_comparison)\n"
      ],
      "metadata": {
        "id": "0DdTmZcJwZq_",
        "outputId": "2bc97808-7310-4970-9fc1-94e3f0c16eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'log_precision' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-207-de75911c5900>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;34m\"K-Nearest Neighbors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Decision Tree\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Deep Neural Network\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     ],\n\u001b[0;32m---> 10\u001b[0;31m     \"Precision\": [log_precision, rf_precision, svm_precision,\n\u001b[0m\u001b[1;32m     11\u001b[0m                   \u001b[0mxgb_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbm_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlgbm_precision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   nb_precision, knn_precision, dt_precision, dnn_precision],\n",
            "\u001b[0;31mNameError\u001b[0m: name 'log_precision' is not defined"
          ]
        }
      ]
    }
  ]
}