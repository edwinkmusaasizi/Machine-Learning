{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNPxZtj6IvHkSSk8XSSzRIO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **0.0 DATA PROCESSING**"
      ],
      "metadata": {
        "id": "qkMtfHS29wQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h4fuIGZJTyAw",
        "outputId": "658cc8ca-7e79-40d6-a2a5-fb62b7d66439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning'...\n",
            "remote: Enumerating objects: 153, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 153 (delta 87), reused 7 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (153/153), 896.63 KiB | 5.82 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/edwinkmusaasizi/Machine-Learning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Machine-Learning\n",
        "%cd data\n",
        "%cd interim\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOq1g9EKBMoS",
        "outputId": "a792cb6d-3b8b-4bae-b007-dba89db83dcd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Machine-Learning\n",
            "/content/Machine-Learning/data\n",
            "/content/Machine-Learning/data/interim\n",
            "cleaned_mental_health_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.1 Data Processing"
      ],
      "metadata": {
        "id": "w3CAYEIG383K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"cleaned_mental_health_data.csv\")\n",
        "\n",
        "# Define adherence labels based on questionnaire responses\n",
        "non_adherence_columns = [\n",
        "    \"Do you ever forget to take your medication?\",\n",
        "    \"Are you careless at times about taking your medication?\",\n",
        "    \"When you feel better, do you sometimes stop taking your medication?\",\n",
        "    \"Sometimes if you feel worse when you take the medication, do you stop taking it?\",\n",
        "    \"I take my medication only when I am sick\"\n",
        "]\n",
        "\n",
        "df[\"adherence\"] = np.where(df[non_adherence_columns].eq(\"Yes\").any(axis=1), 0, 1)\n",
        "\n",
        "# Drop redundant columns\n",
        "df = df.drop(columns=non_adherence_columns + [\"If you have any further comments about medication or this questionnaire, please write them below\"])\n",
        "\n",
        "# Identify all categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "print(\"Categorical columns to encode:\", categorical_cols)\n",
        "\n",
        "# Encode all categorical features\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Split features and labels\n",
        "X = df.drop(columns=\"adherence\").values\n",
        "y = df[\"adherence\"].values\n",
        "\n",
        "# Split data into train, validation, test (70-15-15)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "# Apply SMOTE to only the training set\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE only to the training data\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check new class distribution\n",
        "from collections import Counter\n",
        "print(\"New class distribution:\", Counter(y_train_resampled))\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.FloatTensor(y_val))\n",
        "test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.FloatTensor(y_test))\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJM_oZTZ5Kyy",
        "outputId": "58a4b35d-92ff-46ac-efca-14e31665efe7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical columns to encode: Index(['sex', 'Religion', 'marital status', 'education status', 'residence',\n",
            "       'substance use', 'comorbidity',\n",
            "       'It is unnatural for my mind and body to be controlled by medication?',\n",
            "       'My thoughts are clearer on medication',\n",
            "       'By staying on medication, I can prevent getting sick',\n",
            "       'I feel weird, like a ‘zombie’ on medication',\n",
            "       'Medication makes me feel tired and sluggish',\n",
            "       'Some of your symptoms are made by your mind.', 'You are mentally well',\n",
            "       'You do not need medication', 'Your stay in the hospital is necessary',\n",
            "       'The doctor is right in prescribing medication for you.',\n",
            "       'You do not need to be seen by a doctor or psychiatrist',\n",
            "       'If someone said you have a nervous or mental illness, they would be right',\n",
            "       'None of the unusual things you are experiencing are due to an illness.',\n",
            "       '. Loss of energy or drive', 'Feeling unmotivated or numb',\n",
            "       'Daytime sedation or drowsiness', 'Sleeping too much',\n",
            "       'Muscles being too tense or stiff', 'Muscles trembling or shaking',\n",
            "       'Feeling restless or jittery',\n",
            "       'Need to move around and pace; inability to sit still',\n",
            "       'Trouble getting to sleep or staying asleep (insomnia)',\n",
            "       'Blurry vision', 'Dry mouth', 'Drooling',\n",
            "       'Memory and concentration problems', 'Constipation', 'Weight changes',\n",
            "       'Changes in sexual functioning', 'Menstrual or breast problem',\n",
            "       'I feel over burdened by the number of pills i swallow per day.',\n",
            "       'How often do yo take your drugs',\n",
            "       'How often do you find no medications in the hospital',\n",
            "       'I am satisfied with doctors explanation about mental illness and the need for treatment',\n",
            "       'Do you sometimes stop your medications because of religious or cultural beliefs'],\n",
            "      dtype='object')\n",
            "New class distribution: Counter({np.int64(1): 55, np.int64(0): 55})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution in the training set\n",
        "class_distribution = np.bincount(y_train)\n",
        "print(\"Class Distribution in Training Set:\")\n",
        "print(f\"Class 0 (Non-Adherent): {class_distribution[0]}\")\n",
        "print(f\"Class 1 (Adherent): {class_distribution[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uemTnvGF5Wxd",
        "outputId": "e6153368-0c2a-4664-dd93-9843645e87c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Distribution in Training Set:\n",
            "Class 0 (Non-Adherent): 55\n",
            "Class 1 (Adherent): 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement class weight"
      ],
      "metadata": {
        "id": "zSgD2y_45fQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Define loss function with class weights\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Example model (Assuming a simple neural network)\n",
        "class AdherenceModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(AdherenceModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 2)  # Output has 2 classes (0 and 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)  # No softmax needed for CrossEntropyLoss\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_size = X_train.shape[1]  # Number of features\n",
        "model = AdherenceModel(input_size)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop (simplified)\n",
        "for epoch in range(10):  # Adjust epochs as needed\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        batch_y = batch_y.long()  # Convert to long for CrossEntropyLoss\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlfiL7ZA5ide",
        "outputId": "a596532b-8121-461c-9d29-51f23f19a180"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6996570229530334\n",
            "Epoch 2, Loss: 0.6497613191604614\n",
            "Epoch 3, Loss: 0.7451812028884888\n",
            "Epoch 4, Loss: 0.6076462864875793\n",
            "Epoch 5, Loss: 0.6371175050735474\n",
            "Epoch 6, Loss: 0.5758423805236816\n",
            "Epoch 7, Loss: 0.5178319811820984\n",
            "Epoch 8, Loss: 0.4932590425014496\n",
            "Epoch 9, Loss: 0.5417650938034058\n",
            "Epoch 10, Loss: 0.4978305399417877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M6xRzM2l5T3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL IMPLEMANTION"
      ],
      "metadata": {
        "id": "8jo8AyjIWUam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement Logisitic Regretion"
      ],
      "metadata": {
        "id": "KlciCDP2a9ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Import Libraries"
      ],
      "metadata": {
        "id": "UbYCxxEFcj16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming data preprocessing has been done and the data is available in these variables\n",
        "# X_train, X_val, X_test, y_train, y_val, y_test\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "xV1CAogFbJLk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Training"
      ],
      "metadata": {
        "id": "xN9ha_HTcv_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Logistic Regression model\n",
        "logreg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "logreg_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = end_time - start_time\n"
      ],
      "metadata": {
        "id": "oZZWnBw6c12n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Evaluation"
      ],
      "metadata": {
        "id": "deejg0w_c3h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_logreg = logreg_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_logreg = (y_prob_logreg >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "logreg_precision = precision_score(y_test, y_pred_logreg)\n",
        "logreg_recall = recall_score(y_test, y_pred_logreg)\n",
        "logreg_f1 = f1_score(y_test, y_pred_logreg)\n",
        "logreg_auc = roc_auc_score(y_test, y_prob_logreg)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "model_results = {\n",
        "    \"Model\": [\"Logistic Regression\"],\n",
        "    \"Precision\": [logreg_precision],\n",
        "    \"Recall\": [logreg_recall],\n",
        "    \"F1-Score\": [logreg_f1],\n",
        "    \"AUC\": [logreg_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(model_results)\n",
        "\n",
        "# Display results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwXwqxDM_XKy",
        "outputId": "ab045236-5ac7-4f3d-e534-96f9e58ddbbf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0  Logistic Regression       0.25  0.166667       0.2  0.347222   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.030355  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Support Vector Machine"
      ],
      "metadata": {
        "id": "RYj2KN_odCZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming data preprocessing has been done and the data is available in these variables\n",
        "# X_train, X_val, X_test, y_train, y_val, y_test\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Initialize the SVM model with a radial basis function kernel\n",
        "svm_model = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = end_time - start_time\n"
      ],
      "metadata": {
        "id": "rweuNg7udRko"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "Z9kyZ_mNde_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities\n",
        "y_prob_svm = svm_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_svm = (y_prob_svm >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "svm_precision = precision_score(y_test, y_pred_svm)\n",
        "svm_recall = recall_score(y_test, y_pred_svm)\n",
        "svm_f1 = f1_score(y_test, y_pred_svm)\n",
        "svm_auc = roc_auc_score(y_test, y_prob_svm)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "svm_results = {\n",
        "    \"Model\": [\"Support Vector Machine (SVM)\"],\n",
        "    \"Precision\": [svm_precision],\n",
        "    \"Recall\": [svm_recall],\n",
        "    \"F1-Score\": [svm_f1],\n",
        "    \"AUC\": [svm_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "svm_results_df = pd.DataFrame(svm_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, svm_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX1refb4_2Td",
        "outputId": "08a5ee8f-8b25-4205-80ba-826eab6d11fb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression       0.25  0.166667       0.2  0.347222   \n",
            "1  Support Vector Machine (SVM)       0.00  0.000000       0.0  0.416667   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.030355  \n",
            "1           0.003695  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Random Forest Classifier"
      ],
      "metadata": {
        "id": "Gxv7fW3_dqK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming data preprocessing has been done and the data is available in these variables\n",
        "# X_train, X_val, X_test, y_train, y_val, y_test\n",
        "# Scaling features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "#TRAINING\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the training time\n",
        "training_time = end_time - start_time\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_6Sq7pKDd_kO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "PK6DKYoIAtWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities\n",
        "y_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_rf = (y_prob_rf >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "rf_precision = precision_score(y_test, y_pred_rf)\n",
        "rf_recall = recall_score(y_test, y_pred_rf)\n",
        "rf_f1 = f1_score(y_test, y_pred_rf)\n",
        "rf_auc = roc_auc_score(y_test, y_prob_rf)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "rf_results = {\n",
        "    \"Model\": [\"Random Forest\"],\n",
        "    \"Precision\": [rf_precision],\n",
        "    \"Recall\": [rf_recall],\n",
        "    \"F1-Score\": [rf_f1],\n",
        "    \"AUC\": [rf_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "rf_results_df = pd.DataFrame(rf_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, rf_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAyJ1TijAmpX",
        "outputId": "778bc9c7-dd7c-4e4e-fc3f-93ee25cfec07"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression       0.25  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)       0.00  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest       1.00  0.166667  0.285714  0.465278   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.030355  \n",
            "1           0.003695  \n",
            "2           0.125010  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing a K-Nearest Neighbor"
      ],
      "metadata": {
        "id": "VC7wLDMjfDAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import time\n",
        "\n",
        "# Initialize KNN model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate training time\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_knn = knn_model.predict(X_test_scaled)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "89j6P0lBfPE2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predicted probabilities\n",
        "y_prob_knn = knn_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_knn = (y_prob_knn >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "knn_precision = precision_score(y_test, y_pred_knn)\n",
        "knn_recall = recall_score(y_test, y_pred_knn)\n",
        "knn_f1 = f1_score(y_test, y_pred_knn)\n",
        "knn_auc = roc_auc_score(y_test, y_prob_knn)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "knn_results = {\n",
        "    \"Model\": [\"K-Nearest Neighbors (KNN)\"],\n",
        "    \"Precision\": [knn_precision],\n",
        "    \"Recall\": [knn_recall],\n",
        "    \"F1-Score\": [knn_f1],\n",
        "    \"AUC\": [knn_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "knn_results_df = pd.DataFrame(knn_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, knn_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHl2ICDqBh_n",
        "outputId": "5252a716-a522-4517-ff49-5f528a7d81a1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.030355  \n",
            "1           0.003695  \n",
            "2           0.125010  \n",
            "3           0.000897  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Gradient Boost machine"
      ],
      "metadata": {
        "id": "YEVJvZG7fUYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Initialize Gradient Boosting Classifier\n",
        "gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "gbm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on the validation and test sets\n",
        "y_val_pred = gbm_model.predict(X_val_scaled)\n",
        "y_test_pred = gbm_model.predict(X_test_scaled)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_knn = knn_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_knn = (y_prob_knn >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "knn_precision = precision_score(y_test, y_pred_knn)\n",
        "knn_recall = recall_score(y_test, y_pred_knn)\n",
        "knn_f1 = f1_score(y_test, y_pred_knn)\n",
        "knn_auc = roc_auc_score(y_test, y_prob_knn)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "knn_results = {\n",
        "    \"Model\": [\"K-Nearest Neighbors (KNN)\"],\n",
        "    \"Precision\": [knn_precision],\n",
        "    \"Recall\": [knn_recall],\n",
        "    \"F1-Score\": [knn_f1],\n",
        "    \"AUC\": [knn_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "knn_results_df = pd.DataFrame(knn_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, knn_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnfUNlupf0Vn",
        "outputId": "68ceb97e-5ff1-411c-f992-82ce3e9f73da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "4     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.030355  \n",
            "1           0.003695  \n",
            "2           0.125010  \n",
            "3           0.000897  \n",
            "4           0.122181  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing XGBoost"
      ],
      "metadata": {
        "id": "ODQ1xV3bf3dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlYAPJOsgTrO",
        "outputId": "41714f01-6a1f-4aba-f6b2-13ee90dffa8e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgboost_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "xgboost_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on the validation and test sets\n",
        "y_val_pred = xgboost_model.predict(X_val_scaled)\n",
        "y_test_pred = xgboost_model.predict(X_test_scaled)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_xgb = xgboost_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_xgb = (y_prob_xgb >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "xgb_precision = precision_score(y_test, y_pred_xgb)\n",
        "xgb_recall = recall_score(y_test, y_pred_xgb)\n",
        "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
        "xgb_auc = roc_auc_score(y_test, y_prob_xgb)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "xgb_results = {\n",
        "    \"Model\": [\"XGBoost\"],\n",
        "    \"Precision\": [xgb_precision],\n",
        "    \"Recall\": [xgb_recall],\n",
        "    \"F1-Score\": [xgb_f1],\n",
        "    \"AUC\": [xgb_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "xgb_results_df = pd.DataFrame(xgb_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, xgb_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRRCjGpcgcEf",
        "outputId": "9be5de54-d296-4c6c-e60e-4ade5f07c732"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "4     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "5                       XGBoost   0.200000  0.166667  0.181818  0.375000   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.030355  \n",
            "1           0.003695  \n",
            "2           0.125010  \n",
            "3           0.000897  \n",
            "4           0.122181  \n",
            "5           0.088999  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing LightGBM"
      ],
      "metadata": {
        "id": "trCnfvrqhGK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm\n"
      ],
      "metadata": {
        "id": "RrGTq8YBiqyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97fd2796-4ba5-4859-cced-37047e259dea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Initialize LightGBM Classifier\n",
        "lgbm_model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Record the start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "lgbm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on the validation and test sets\n",
        "y_val_pred = lgbm_model.predict(X_val_scaled)\n",
        "y_test_pred = lgbm_model.predict(X_test_scaled)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_lgbm = lgbm_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_lgbm = (y_prob_lgbm >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "lgbm_precision = precision_score(y_test, y_pred_lgbm)\n",
        "lgbm_recall = recall_score(y_test, y_pred_lgbm)\n",
        "lgbm_f1 = f1_score(y_test, y_pred_lgbm)\n",
        "lgbm_auc = roc_auc_score(y_test, y_prob_lgbm)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "lgbm_results = {\n",
        "    \"Model\": [\"LightGBM\"],\n",
        "    \"Precision\": [lgbm_precision],\n",
        "    \"Recall\": [lgbm_recall],\n",
        "    \"F1-Score\": [lgbm_f1],\n",
        "    \"AUC\": [lgbm_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "lgbm_results_df = pd.DataFrame(lgbm_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, lgbm_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "cc2tl1xiisY2",
        "outputId": "4bb62ae0-466b-42e2-8ed6-3e6045c0ed7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 26, number of negative: 55\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000033 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 218\n",
            "[LightGBM] [Info] Number of data points in the train set: 81, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.320988 -> initscore=-0.749237\n",
            "[LightGBM] [Info] Start training from score -0.749237\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "                          Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0           Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1  Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                 Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "4     K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "5                       XGBoost   0.200000  0.166667  0.181818  0.375000   \n",
            "6                      LightGBM   0.250000  0.166667  0.200000  0.458333   \n",
            "\n",
            "   Training Time (s)  \n",
            "0           0.030355  \n",
            "1           0.003695  \n",
            "2           0.125010  \n",
            "3           0.000897  \n",
            "4           0.122181  \n",
            "5           0.088999  \n",
            "6           0.014545  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing CATBoost"
      ],
      "metadata": {
        "id": "kWUT3-V_i3jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.2 catboost==1.2.7\n"
      ],
      "metadata": {
        "id": "gxM03uSvpKjk",
        "outputId": "f9e83605-0f6b-4949-8eb5-c866d2274529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.2\n",
            "  Downloading numpy-1.24.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting catboost==1.2.7\n",
            "  Downloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (3.10.0)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (1.14.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost==1.2.7) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost==1.2.7) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost==1.2.7) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost==1.2.7) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost==1.2.7) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost==1.2.7) (9.0.0)\n",
            "Downloading numpy-1.24.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.7-cp311-cp311-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, catboost\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.2 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.2 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.2 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.2 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.2 which is incompatible.\n",
            "blosc2 3.2.0 requires numpy>=1.26, but you have numpy 1.24.2 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.24.2 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed catboost-1.2.7 numpy-1.24.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "946550e1a9d8448f8b63656b44560bf6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Initialize CatBoost Classifier\n",
        "catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=3, random_seed=42, verbose=0)\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "catboost_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Record end time\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on validation and test sets\n",
        "y_val_pred = catboost_model.predict(X_val_scaled)\n",
        "y_test_pred = catboost_model.predict(X_test_scaled)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_prob_catboost = catboost_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_catboost = (y_prob_catboost >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "catboost_precision = precision_score(y_test, y_pred_catboost)\n",
        "catboost_recall = recall_score(y_test, y_pred_catboost)\n",
        "catboost_f1 = f1_score(y_test, y_pred_catboost)\n",
        "catboost_auc = roc_auc_score(y_test, y_prob_catboost)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "catboost_results = {\n",
        "    \"Model\": [\"CatBoost\"],\n",
        "    \"Precision\": [catboost_precision],\n",
        "    \"Recall\": [catboost_recall],\n",
        "    \"F1-Score\": [catboost_f1],\n",
        "    \"AUC\": [catboost_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "catboost_results_df = pd.DataFrame(catboost_results)\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, catboost_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "A81YYFTgjVP-",
        "outputId": "405721ca-34e0-435e-814f-0f71dc476e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-d38cce442b73>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize CatBoost Classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .core import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mFeaturesData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEFstrType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEShapCalcType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEFeaturesSelectionAlgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEFeaturesSelectionGrouping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mPool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostRanker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCatBoostError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_gaussian_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_have_equal_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_regressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_ranker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiRegressionCustomMetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mMultiRegressionCustomObjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiTargetCustomMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiTargetCustomObjective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplot_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_plot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_plot_offline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOfflineMetricVisualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_catboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBuiltinMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/plot_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_catboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfspath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_catboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36minit _catboost\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing A neural network"
      ],
      "metadata": {
        "id": "Pv6CDU82mFzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# Define the Neural Network (NN) Model\n",
        "class NNModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # Input layer\n",
        "        self.fc2 = nn.Linear(128, 64)         # Hidden layer 1\n",
        "        self.fc3 = nn.Linear(64, 32)          # Hidden layer 2\n",
        "        self.fc4 = nn.Linear(32, 1)           # Output layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.relu(self.fc2(x))  # Apply ReLU activation\n",
        "        x = self.relu(self.fc3(x))  # Apply ReLU activation\n",
        "        x = self.sigmoid(self.fc4(x))  # Output layer with sigmoid for binary classification\n",
        "        return x\n",
        "\n",
        "# Initialize the Neural Network model\n",
        "input_dim = X_train_scaled.shape[1]  # Number of features\n",
        "nn_model = NNModel(input_dim)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "nn_model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Record start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the Neural Network Model (assuming X_train_scaled and y_train are tensors)\n",
        "for epoch in range(100):  # Number of epochs\n",
        "    # Move data to device\n",
        "    inputs = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = nn_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Record end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on validation and test sets\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "y_val_pred = nn_model(X_val_tensor).cpu().detach().numpy()\n",
        "y_val_pred = (y_val_pred > 0.5).astype(int)  # Threshold at 0.5\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "y_test_pred = nn_model(X_test_tensor).cpu().detach().numpy()\n",
        "y_test_pred = (y_test_pred > 0.5).astype(int)  # Threshold at 0.5\n",
        "\n",
        "# Get predicted probabilities\n",
        "nn_model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    # Convert the test data to a tensor\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    y_prob_nn = nn_model(X_test_tensor).cpu().numpy().flatten()  # Probabilities for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_nn = (y_prob_nn >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "nn_precision = precision_score(y_test, y_pred_nn)\n",
        "nn_recall = recall_score(y_test, y_pred_nn)\n",
        "nn_f1 = f1_score(y_test, y_pred_nn)\n",
        "nn_auc = roc_auc_score(y_test, y_prob_nn)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "nn_results = {\n",
        "    \"Model\": [\"Neural Network (NN)\"],\n",
        "    \"Precision\": [nn_precision],\n",
        "    \"Recall\": [nn_recall],\n",
        "    \"F1-Score\": [nn_f1],\n",
        "    \"AUC\": [nn_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "nn_results_df = pd.DataFrame(nn_results)\n",
        "\n",
        "# Initialize an empty DataFrame to hold results if it doesn't exist\n",
        "try:\n",
        "    results_df\n",
        "except NameError:\n",
        "    results_df = pd.DataFrame(columns=[\"Model\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC\", \"Training Time (s)\"])\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, nn_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "VKJXJE1UDn3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing DNN"
      ],
      "metadata": {
        "id": "YXYQSVqtvFPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import pandas as pd\n",
        "\n",
        "# Define the Neural Network Model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.output = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.output(x))\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X_train_scaled.shape[1]  # Number of features\n",
        "model = NeuralNetwork(input_dim)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Prepare data (assuming X_train_scaled, y_train are numpy arrays or tensors)\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "# Initialize the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "start_time = time.time()\n",
        "\n",
        "# Set the model to training mode\n",
        "model.train()\n",
        "for epoch in range(100):  # Training for 100 epochs\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Record the end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "\n",
        "# Make predictions on the validation and test sets\n",
        "with torch.no_grad():\n",
        "    y_val_pred_prob = model(X_val_tensor).squeeze().cpu().numpy()\n",
        "    y_test_pred_prob = model(X_test_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_val_pred = (y_val_pred_prob >= 0.5).astype(int)\n",
        "y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics for validation set\n",
        "val_precision = precision_score(y_val, y_val_pred)\n",
        "val_recall = recall_score(y_val, y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred)\n",
        "val_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
        "\n",
        "# Calculate evaluation metrics for test set\n",
        "test_precision = precision_score(y_test, y_test_pred)\n",
        "test_recall = recall_score(y_test, y_test_pred)\n",
        "test_f1 = f1_score(y_test, y_test_pred)\n",
        "test_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "nn_results = {\n",
        "    \"Model\": [\"Deep Neural Network (NN)\"],\n",
        "    \"Precision\": [test_precision],\n",
        "    \"Recall\": [test_recall],\n",
        "    \"F1-Score\": [test_f1],\n",
        "    \"AUC\": [test_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "nn_results_df = pd.DataFrame(nn_results)\n",
        "\n",
        "# Initialize an empty DataFrame to hold results if it doesn't exist\n",
        "try:\n",
        "    results_df\n",
        "except NameError:\n",
        "    results_df = pd.DataFrame(columns=[\"Model\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC\", \"Training Time (s)\"])\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, nn_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "S6Zf_WuaPtXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Fastforwar-NN"
      ],
      "metadata": {
        "id": "86QiETKbLc-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Define the Feedforward Neural Network (FNN) Model\n",
        "class FNNModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(FNNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)  # Input layer\n",
        "        self.fc2 = nn.Linear(64, 32)         # Hidden layer\n",
        "        self.fc3 = nn.Linear(32, 1)          # Output layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))   # Apply ReLU activation\n",
        "        x = self.relu(self.fc2(x))   # Apply ReLU activation\n",
        "        x = self.sigmoid(self.fc3(x))  # Output layer with sigmoid for binary classification\n",
        "        return x\n",
        "\n",
        "# Initialize the FNN model\n",
        "input_dim = X_train_scaled.shape[1]  # Number of features\n",
        "fnn_model = FNNModel(input_dim)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "fnn_model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(fnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Record start time for training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the FNN Model (assuming X_train_scaled and y_train are tensors)\n",
        "for epoch in range(100):  # Number of epochs\n",
        "    # Move data to device\n",
        "    inputs = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
        "    labels = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = fnn_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Record end time for training\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Predict on validation and test sets\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32).to(device)\n",
        "y_val_pred = fnn_model(X_val_tensor).cpu().detach().numpy()\n",
        "y_val_pred = (y_val_pred > 0.5).astype(int)  # Threshold at 0.5\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "y_test_pred = fnn_model(X_test_tensor).cpu().detach().numpy()\n",
        "y_test_pred = (y_test_pred > 0.5).astype(int)  # Threshold at 0.5\n",
        "\n",
        "# Get predicted probabilities\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    # Convert the test data to a tensor\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    y_prob_fnn = model(X_test_tensor).cpu().numpy().flatten()  # Probabilities for class 1\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_pred_fnn = (y_prob_fnn >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "fnn_precision = precision_score(y_test, y_pred_fnn)\n",
        "fnn_recall = recall_score(y_test, y_pred_fnn)\n",
        "fnn_f1 = f1_score(y_test, y_pred_fnn)\n",
        "fnn_auc = roc_auc_score(y_test, y_prob_fnn)\n",
        "\n",
        "# Calculate training time (assuming it was previously calculated or use current time)\n",
        "start_time = time.time()\n",
        "# model.fit(X_train_scaled, y_train)  # Uncomment and use this if not done yet\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "# Store results in a dictionary for comparison\n",
        "fnn_results = {\n",
        "    \"Model\": [\"Feedforward Neural Network (FNN)\"],\n",
        "    \"Precision\": [fnn_precision],\n",
        "    \"Recall\": [fnn_recall],\n",
        "    \"F1-Score\": [fnn_f1],\n",
        "    \"AUC\": [fnn_auc],\n",
        "    \"Training Time (s)\": [training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "fnn_results_df = pd.DataFrame(fnn_results)\n",
        "\n",
        "# Initialize an empty DataFrame to hold results if it doesn't exist\n",
        "try:\n",
        "    results_df\n",
        "except NameError:\n",
        "    results_df = pd.DataFrame(columns=[\"Model\", \"Precision\", \"Recall\", \"F1-Score\", \"AUC\", \"Training Time (s)\"])\n",
        "\n",
        "# Append to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, fnn_results_df], ignore_index=True)\n",
        "\n",
        "# Display updated results\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "RsFxLw-MLuoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing The Models"
      ],
      "metadata": {
        "id": "gRKUKYlMwVEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the DataFrame by 'AUC' in descending order\n",
        "sorted_df = results_df.sort_values(by='AUC', ascending=False)\n",
        "\n",
        "# Display the sorted DataFrame\n",
        "sorted_df\n"
      ],
      "metadata": {
        "id": "FQd3w0GjQ1_x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "19fdac27-ae22-487f-8805-b1a6479a8d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               Model  Precision    Recall  F1-Score       AUC  \\\n",
              "3          K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
              "4          K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
              "2                      Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
              "6                           LightGBM   0.250000  0.166667  0.200000  0.458333   \n",
              "7                           CatBoost   0.250000  0.166667  0.200000  0.416667   \n",
              "1       Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
              "8                Neural Network (NN)   0.250000  0.166667  0.200000  0.416667   \n",
              "9           Deep Neural Network (NN)   0.333333  0.166667  0.222222  0.388889   \n",
              "10  Feedforward Neural Network (FNN)   0.333333  0.166667  0.222222  0.388889   \n",
              "5                            XGBoost   0.200000  0.166667  0.181818  0.375000   \n",
              "0                Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
              "\n",
              "    Training Time (s)  \n",
              "3            0.001327  \n",
              "4            0.203899  \n",
              "2            0.228961  \n",
              "6            0.073827  \n",
              "7            0.186791  \n",
              "1            0.011965  \n",
              "8            0.293892  \n",
              "9            0.216843  \n",
              "10           0.000035  \n",
              "5            0.103569  \n",
              "0            0.034215  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-49d3bb93-88d6-466d-be09-b8eda8029357\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>AUC</th>\n",
              "      <th>Training Time (s)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>K-Nearest Neighbors (KNN)</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.645833</td>\n",
              "      <td>0.001327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>K-Nearest Neighbors (KNN)</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.645833</td>\n",
              "      <td>0.203899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.465278</td>\n",
              "      <td>0.228961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LightGBM</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.458333</td>\n",
              "      <td>0.073827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>CatBoost</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.186791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Support Vector Machine (SVM)</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.011965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Neural Network (NN)</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.293892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Deep Neural Network (NN)</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.216843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Feedforward Neural Network (FNN)</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.000035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>0.103569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.347222</td>\n",
              "      <td>0.034215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-49d3bb93-88d6-466d-be09-b8eda8029357')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-49d3bb93-88d6-466d-be09-b8eda8029357 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-49d3bb93-88d6-466d-be09-b8eda8029357');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ddaa3392-fad1-41ec-87d2-9b4b7a27fadf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ddaa3392-fad1-41ec-87d2-9b4b7a27fadf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ddaa3392-fad1-41ec-87d2-9b4b7a27fadf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_fd2a38a0-684f-4c31-98b7-7e5c5d9561f4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('sorted_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_fd2a38a0-684f-4c31-98b7-7e5c5d9561f4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('sorted_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sorted_df",
              "summary": "{\n  \"name\": \"sorted_df\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"XGBoost\",\n          \"Random Forest\",\n          \"Neural Network (NN)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28239595384402155,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.6666666666666666,\n          1.0,\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08989331499509892,\n        \"min\": 0.0,\n        \"max\": 0.3333333333333333,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.3333333333333333,\n          0.16666666666666666,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1-Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12392048031733267,\n        \"min\": 0.0,\n        \"max\": 0.4444444444444444,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.4444444444444444,\n          0.2857142857142857,\n          0.18181818181818182\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10196752690543141,\n        \"min\": 0.3472222222222222,\n        \"max\": 0.6458333333333334,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.37500000000000006,\n          0.4652777777777778,\n          0.41666666666666663\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Training Time (s)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1061996243350137,\n        \"min\": 3.528594970703125e-05,\n        \"max\": 0.29389238357543945,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.011964797973632812,\n          0.0013267993927001953,\n          0.10356903076171875\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving"
      ],
      "metadata": {
        "id": "fzVxB4U5Q7EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the sorted DataFrame as a CSV file in the current directory\n",
        "sorted_df.to_csv('sorted_model_comparison.csv', index=False)\n",
        "\n",
        "# Optionally, display a message indicating the file has been saved\n",
        "print(\"The sorted model comparison has been saved as 'sorted_model_comparison.csv'\")\n"
      ],
      "metadata": {
        "id": "8EUK4b9wQ8GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19147f08-8b71-4204-9090-31f1d2f25199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sorted model comparison has been saved as 'sorted_model_comparison.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembling\n",
        "\n",
        "Using a Voting Ensemble"
      ],
      "metadata": {
        "id": "73fQdABuRSKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Initialize the base models\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "lgbm_model = LGBMClassifier()\n",
        "catboost_model = CatBoostClassifier(silent=True)\n",
        "\n",
        "# Create the ensemble model using soft voting\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('rf', rf_model),\n",
        "    ('knn', knn_model),\n",
        "    ('xgb', xgb_model),\n",
        "    ('lgbm', lgbm_model),\n",
        "    ('catboost', catboost_model)\n",
        "], voting='soft')\n",
        "\n",
        "# Train the ensemble model\n",
        "start_time = time.time()\n",
        "ensemble_model.fit(X_train_scaled, y_train)\n",
        "end_time = time.time()\n",
        "ensemble_training_time = end_time - start_time\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_prob_ensemble = ensemble_model.predict_proba(X_test_scaled)[:, 1]  # Probability for class 1\n",
        "y_pred_ensemble = (y_prob_ensemble >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics for the ensemble model\n",
        "ensemble_precision = precision_score(y_test, y_pred_ensemble)\n",
        "ensemble_recall = recall_score(y_test, y_pred_ensemble)\n",
        "ensemble_f1 = f1_score(y_test, y_pred_ensemble)\n",
        "ensemble_auc = roc_auc_score(y_test, y_prob_ensemble)\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"Ensemble Model Evaluation:\")\n",
        "print(f\"Precision: {ensemble_precision:.4f} | Recall: {ensemble_recall:.4f}\")\n",
        "print(f\"F1-Score: {ensemble_f1:.4f} | AUC: {ensemble_auc:.4f}\")\n",
        "print(f\"Training Time: {ensemble_training_time:.4f} seconds\")\n",
        "\n",
        "# Store the results for comparison\n",
        "ensemble_results = {\n",
        "    \"Model\": [\"Ensemble Model (Voting)\"],\n",
        "    \"Precision\": [ensemble_precision],\n",
        "    \"Recall\": [ensemble_recall],\n",
        "    \"F1-Score\": [ensemble_f1],\n",
        "    \"AUC\": [ensemble_auc],\n",
        "    \"Training Time (s)\": [ensemble_training_time]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "ensemble_results_df = pd.DataFrame(ensemble_results)\n",
        "\n",
        "# Append the ensemble results to the existing results DataFrame\n",
        "results_df = pd.concat([results_df, ensemble_results_df], ignore_index=True)\n",
        "\n",
        "# Display the updated results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEIGmstctT6H",
        "outputId": "053867ca-3f5e-4d2f-b64c-5b3fcb1e96bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [09:34:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 26, number of negative: 55\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000115 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 218\n",
            "[LightGBM] [Info] Number of data points in the train set: 81, number of used features: 45\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.320988 -> initscore=-0.749237\n",
            "[LightGBM] [Info] Start training from score -0.749237\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Ensemble Model Evaluation:\n",
            "Precision: 0.2000 | Recall: 0.1667\n",
            "F1-Score: 0.1818 | AUC: 0.4583\n",
            "Training Time: 2.8643 seconds\n",
            "                               Model  Precision    Recall  F1-Score       AUC  \\\n",
            "0                Logistic Regression   0.250000  0.166667  0.200000  0.347222   \n",
            "1       Support Vector Machine (SVM)   0.000000  0.000000  0.000000  0.416667   \n",
            "2                      Random Forest   1.000000  0.166667  0.285714  0.465278   \n",
            "3          K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "4          K-Nearest Neighbors (KNN)   0.666667  0.333333  0.444444  0.645833   \n",
            "5                            XGBoost   0.200000  0.166667  0.181818  0.375000   \n",
            "6                           LightGBM   0.250000  0.166667  0.200000  0.458333   \n",
            "7                           CatBoost   0.250000  0.166667  0.200000  0.416667   \n",
            "8                Neural Network (NN)   0.250000  0.166667  0.200000  0.416667   \n",
            "9           Deep Neural Network (NN)   0.333333  0.166667  0.222222  0.388889   \n",
            "10  Feedforward Neural Network (FNN)   0.333333  0.166667  0.222222  0.388889   \n",
            "11           Ensemble Model (Voting)   0.200000  0.166667  0.181818  0.458333   \n",
            "12           Ensemble Model (Voting)   0.200000  0.166667  0.181818  0.458333   \n",
            "\n",
            "    Training Time (s)  \n",
            "0            0.034215  \n",
            "1            0.011965  \n",
            "2            0.228961  \n",
            "3            0.001327  \n",
            "4            0.203899  \n",
            "5            0.103569  \n",
            "6            0.073827  \n",
            "7            0.186791  \n",
            "8            0.293892  \n",
            "9            0.216843  \n",
            "10           0.000035  \n",
            "11           6.774139  \n",
            "12           2.864307  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Sparse Knowledge Distillation (CSDK)"
      ],
      "metadata": {
        "id": "3AaJi3sP6rcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Define the Student Model"
      ],
      "metadata": {
        "id": "QZwFlL3D64es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a smaller student model\n",
        "class StudentModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(StudentModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)  # Reduced number of neurons\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.output = nn.Linear(16, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.output(x))\n",
        "        return x\n",
        "\n",
        "# Initialize the student model\n",
        "input_dim = X_train_scaled.shape[1]  # Number of features\n",
        "student_model = StudentModel(input_dim)\n"
      ],
      "metadata": {
        "id": "41DuRr967fTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Contrastive Sparse Knowledge Distillation Loss"
      ],
      "metadata": {
        "id": "An_7d4QF7h9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(student_outputs, teacher_outputs, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Contrastive loss function for knowledge distillation.\n",
        "    \"\"\"\n",
        "    # Compute softmax probabilities for teacher and student outputs\n",
        "    student_probs = nn.Softmax(dim=1)(student_outputs / temperature)\n",
        "    teacher_probs = nn.Softmax(dim=1)(teacher_outputs / temperature)\n",
        "\n",
        "    # Calculate the Kullback-Leibler divergence between teacher and student\n",
        "    loss = nn.KLDivLoss(reduction='batchmean')(student_probs.log(), teacher_probs)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "bgEPsx2N7mwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Knowledge Distillation Training Loop"
      ],
      "metadata": {
        "id": "TVPviPQb7pxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer for the student model\n",
        "criterion = contrastive_loss  # Using contrastive loss\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100  # Set the number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    student_model.train()\n",
        "\n",
        "    # Make predictions with the teacher model (voting ensemble)\n",
        "    with torch.no_grad():\n",
        "        teacher_outputs = ensemble_model.predict_proba(X_train_scaled)  # Get teacher probabilities\n",
        "        teacher_outputs = torch.tensor(teacher_outputs).float()  # Convert to tensor\n",
        "\n",
        "    # Convert training data to tensors\n",
        "    inputs = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "    labels = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass for the student model\n",
        "    student_outputs = student_model(inputs)\n",
        "\n",
        "    # Compute the contrastive loss between student and teacher outputs\n",
        "    loss = criterion(student_outputs, teacher_outputs)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "1kK1RZL37sZK",
        "outputId": "b50f650d-5f75-4769-8aba-6dff8b9094a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [60/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [70/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [80/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [90/100], Loss: -0.6381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Evaluate the Student Model"
      ],
      "metadata": {
        "id": "kq4o0Rqm7xeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on the test set\n",
        "student_model.eval()\n",
        "\n",
        "# Get predictions for the test set\n",
        "with torch.no_grad():\n",
        "    y_test_pred_prob = student_model(torch.tensor(X_test_scaled, dtype=torch.float32)).squeeze().cpu().numpy()\n",
        "\n",
        "# Convert probabilities to binary predictions (threshold = 0.5)\n",
        "y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics for the student model\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "student_precision = precision_score(y_test, y_test_pred)\n",
        "student_recall = recall_score(y_test, y_test_pred)\n",
        "student_f1 = f1_score(y_test, y_test_pred)\n",
        "student_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Student Model Evaluation:\")\n",
        "print(f\"Precision: {student_precision:.4f} | Recall: {student_recall:.4f}\")\n",
        "print(f\"F1-Score: {student_f1:.4f} | AUC: {student_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "3_F_9r0771Ui",
        "outputId": "d32f4e97-21ec-4606-d9d0-1047028ff72a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Model Evaluation:\n",
            "Precision: 0.4000 | Recall: 0.3333\n",
            "F1-Score: 0.3636 | AUC: 0.6111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Neuroevolution"
      ],
      "metadata": {
        "id": "v9Lv-XAO89LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5.1: Define the Genetic Algorithm"
      ],
      "metadata": {
        "id": "-1L44Z1i8_4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import time\n",
        "\n",
        "# Define the neural network model\n",
        "class EvolvingNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1=64, hidden_dim2=32):\n",
        "        super(EvolvingNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        self.output = nn.Linear(hidden_dim2, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.output(x))\n",
        "        return x\n",
        "\n",
        "# Fitness function to evaluate the models\n",
        "def fitness_function(model, X_train, y_train, X_val, y_val):\n",
        "    # Train the model\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Convert data to tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_val_pred_prob = model(X_val_tensor).squeeze().numpy()\n",
        "        y_val_pred = (y_val_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    precision = precision_score(y_val, y_val_pred)\n",
        "    recall = recall_score(y_val, y_val_pred)\n",
        "    f1 = f1_score(y_val, y_val_pred)\n",
        "    auc = roc_auc_score(y_val, y_val_pred_prob)\n",
        "\n",
        "    # Return the fitness score (F1-Score as an example)\n",
        "    return f1\n",
        "\n",
        "# Selection of parents based on fitness scores\n",
        "def select_parents(population, X_train, y_train, X_val, y_val):\n",
        "    # Evaluate fitness for each model in the population\n",
        "    fitness_scores = [fitness_function(model, X_train, y_train, X_val, y_val) for model in population]\n",
        "\n",
        "    # Zip fitness scores with models and sort by fitness score\n",
        "    sorted_population = [model for _, model in sorted(zip(fitness_scores, population), key=lambda x: x[0], reverse=True)]\n",
        "\n",
        "    return sorted_population[:2]  # Select the top 2 models\n",
        "\n",
        "# Crossover function (genetic crossover to create new models)\n",
        "def crossover(parent1, parent2):\n",
        "    # Create new model by combining parts of parent1 and parent2\n",
        "    new_model = EvolvingNN(input_dim=parent1.fc1.in_features)  # Initialize a new model\n",
        "    new_model.fc1.weight.data = (parent1.fc1.weight.data + parent2.fc1.weight.data) / 2\n",
        "    new_model.fc2.weight.data = (parent1.fc2.weight.data + parent2.fc2.weight.data) / 2\n",
        "    new_model.output.weight.data = (parent1.output.weight.data + parent2.output.weight.data) / 2\n",
        "    return new_model\n",
        "\n",
        "# Mutation function (randomly mutate the model weights)\n",
        "def mutate(model, mutation_rate=0.01):\n",
        "    # Apply mutation to the model's weights\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            if random.random() < mutation_rate:\n",
        "                param += torch.randn_like(param) * 0.1  # Random mutation\n",
        "    return model\n",
        "\n",
        "# Neuroevolution algorithm\n",
        "def neuroevolution(X_train, y_train, X_val, y_val, input_dim, population_size=10, generations=5):\n",
        "    # Initialize a population of random models\n",
        "    population = [EvolvingNN(input_dim) for _ in range(population_size)]\n",
        "\n",
        "    # Evolve over generations\n",
        "    for generation in range(generations):\n",
        "        print(f\"Generation {generation + 1}/{generations}\")\n",
        "\n",
        "        # Select parents based on fitness\n",
        "        parents = select_parents(population, X_train, y_train, X_val, y_val)\n",
        "\n",
        "        # Create the next generation\n",
        "        next_generation = []\n",
        "        for _ in range(population_size // 2):  # Create pairs of parents\n",
        "            parent1, parent2 = random.sample(parents, 2)\n",
        "            child1 = crossover(parent1, parent2)\n",
        "            child2 = crossover(parent2, parent1)\n",
        "            next_generation.extend([child1, child2])\n",
        "\n",
        "        # Apply mutation to the next generation\n",
        "        next_generation = [mutate(model) for model in next_generation]\n",
        "\n",
        "        # Replace the population with the next generation\n",
        "        population = next_generation\n",
        "\n",
        "    # Evaluate the fitness of the final population\n",
        "    final_fitness_scores = [fitness_function(model, X_train, y_train, X_val, y_val) for model in population]\n",
        "    best_model = population[np.argmax(final_fitness_scores)]  # Best model based on fitness\n",
        "    return best_model\n",
        "\n",
        "# Example usage (Make sure to use the correct input data)\n",
        "# X_train_scaled, y_train, X_val_scaled, y_val should be the preprocessed training and validation data\n",
        "best_model = neuroevolution(X_train_scaled, y_train, X_val_scaled, y_val, input_dim=X_train_scaled.shape[1])\n",
        "\n",
        "# Evaluate the best evolved model on the test set\n",
        "best_model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "    y_test_pred_prob = best_model(X_test_tensor).squeeze().cpu().numpy()\n",
        "    y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "# Evaluate the performance on test data\n",
        "precision = precision_score(y_test, y_test_pred)\n",
        "recall = recall_score(y_test, y_test_pred)\n",
        "f1 = f1_score(y_test, y_test_pred)\n",
        "auc = roc_auc_score(y_test, y_test_pred_prob)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(f\"AUC: {auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "rKDeutqT9DnY",
        "outputId": "3829184d-61cb-45e0-ad17-37d52b3e36c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.3529\n",
            "Recall: 1.0000\n",
            "F1-Score: 0.5217\n",
            "AUC: 0.5417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "saving"
      ],
      "metadata": {
        "id": "0Qv9vmiyKn-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best student model\n",
        "torch.save(student_model.state_dict(), \"best_student_model.pth\")\n",
        "print(\"Student model saved successfully!\")\n"
      ],
      "metadata": {
        "id": "ktDDHK4RKpkF",
        "outputId": "7aa513e8-b6de-41f1-fb43-f91b0996e7a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#files.download(\"best_student_model.pth\")  # Or best_student_model.pt for TorchScript\n"
      ],
      "metadata": {
        "id": "DCum_dPHK4cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable AI (AI)"
      ],
      "metadata": {
        "id": "AO0ToXEsDvNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lime shap captum\n"
      ],
      "metadata": {
        "id": "-ObwDk-FmA-w",
        "outputId": "ace6631d-dab1-43bf-b7ca-bb09eab90142",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m204.8/275.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.47.0)\n",
            "Collecting captum\n",
            "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (1.24.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.14.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.12.2)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from captum) (2.6.0+cu124)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.4.2)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.1.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.3.13)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.18.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6->captum) (3.0.2)\n",
            "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=35df4d277350af98571a2ad0b190c8135b07686cf12d78728711d825469c22eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lime, captum\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed captum-0.7.0 lime-0.2.0.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib\n"
      ],
      "metadata": {
        "id": "eQoEj8_mzcyB",
        "outputId": "e8355752-58eb-41c7-e778-9d68cffc3a1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Assuming your models are stored in the following variables:\n",
        "# rf_model - Random Forest\n",
        "# knn_model - K-Nearest Neighbors\n",
        "# svm_model - Support Vector Machine\n",
        "# lgbm_model - LightGBM\n",
        "\n",
        "# Save each model to a file using joblib\n",
        "joblib.dump(rf_model, 'random_forest_model.pkl')    # Save Random Forest model\n",
        "joblib.dump(knn_model, 'knn_model.pkl')              # Save K-Nearest Neighbors model\n",
        "joblib.dump(svm_model, 'svm_model.pkl')              # Save SVM model\n",
        "joblib.dump(lgbm_model, 'lightgbm_model.pkl')        # Save LightGBM model\n",
        "\n",
        "print(\"Models saved successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "44krbWBhw4cH",
        "outputId": "f41858dd-f183-49e9-ca89-9cb04322d5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save models to Google Drive (optional)\n",
        "joblib.dump(rf_model, '/content/drive/MyDrive/random_forest_model.pkl')\n",
        "joblib.dump(knn_model, '/content/drive/MyDrive/knn_model.pkl')\n",
        "joblib.dump(svm_model, '/content/drive/MyDrive/svm_model.pkl')\n",
        "joblib.dump(lgbm_model, '/content/drive/MyDrive/lightgbm_model.pkl')\n",
        "\n",
        "print(\"Models saved to Google Drive successfully!\")\n"
      ],
      "metadata": {
        "id": "-Z7wBfCJ3V7w",
        "outputId": "df183659-fbc2-4b8d-d552-8258994debf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Models saved to Google Drive successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/edwinkmusaasizi/Machine-Learning.git\n"
      ],
      "metadata": {
        "id": "AsmBrOk63rVV",
        "outputId": "800f4a90-0343-4d71-a111-d20458699bac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Machine-Learning'...\n",
            "remote: Enumerating objects: 153, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 153 (delta 87), reused 7 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (153/153), 896.63 KiB | 6.10 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "N0h1fHDT5Dpy",
        "outputId": "3bd8e6fa-7a7d-4947-8e14-4cb516d05e0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cleaned_mental_health_data.csv\tlightgbm_model.pkl  random_forest_model.pkl\n",
            "knn_model.pkl\t\t\tMachine-Learning    svm_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "EwLnmmJ_1-Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8GxmbEKZ2Dgu"
      }
    }
  ]
}